{
  "project": {
    "name": "francoishup-2048",
    "description": {
      "summary": "This project implements the 2048 game along with both a dummy AI and a Deep Q-Network (DQN)-based AI. It includes a Tkinter GUI for playing and training the AI model.",
      "problem_solved": "Demonstrates how reinforcement learning (DQN) can be applied to a discrete puzzle game (2048) to achieve improved gameplay performance compared to a naive agent.",
      "impact": "The DQN agent is able to learn better strategies for merging tiles and achieving higher scores autonomously, illustrating how RL can outperform simple heuristics in tile-based puzzle environments.",
      "technologies": [
        "Python",
        "PyTorch",
        "tkinter",
        "NumPy",
        "Matplotlib",
        "TQDM"
      ],
      "role": "Developer/Contributor: integrated the game engine, GUI, and DQN agent training loops.",
      "challenges": [
        "Implementing a stable DQN for a 4x4 board game state representation.",
        "Designing an efficient replay memory strategy and ensuring valid action selection in 2048â€™s puzzle mechanics.",
        "Providing a user-friendly training UI in Tkinter and integrating real-time metrics."
      ],
      "relevance": "Showcases experience with reinforcement learning, Python, and user interface design, demonstrating the end-to-end workflow of building and training an AI model for a puzzle game."
    },
    "architecture": {
      "overview": "The system is organized into modules for the game logic (GameEngine), user interface (Game2048GUI), and agents (dummy or DQN). The DQN agent handles training and inference, while the Tkinter UI allows human play and monitoring of training progress.",
      "components": [
        {
          "name": "GameEngine",
          "description": "Manages the core 2048 game state (grid, score, valid moves) and provides step/reset methods."
        },
        {
          "name": "Game2048GUI",
          "description": "Tkinter-based graphical interface allowing users to play, record games, load models, and run training sessions."
        },
        {
          "name": "DQN2048",
          "description": "Implements a DQN agent for 2048 with policy and target networks, replay memory, training loops, and evaluation methods."
        },
        {
          "name": "DummyAI",
          "description": "A baseline agent that selects moves randomly for comparison with the DQN agent."
        },
        {
          "name": "TrainingWindow / TrainingConfigWindow",
          "description": "Tkinter windows enabling user to configure hyperparameters, start RL training in a background thread, and visualize performance metrics."
        }
      ]
    },
    "technical_details": {
      "design_decisions": [
        {
          "decision": "Use a Dueling DQN network architecture instead of a standard MLP.",
          "reasoning": "The dueling architecture helps learn a separate state-value function and action-specific advantages, improving learning efficiency and stability."
        },
        {
          "decision": "Use replay memory for training the DQN.",
          "reasoning": "Replay memory decorrelates consecutive states and stabilizes training by sampling past experiences, a standard approach in DQN-based methods."
        }
      ],
      "performance_optimizations": [
        {
          "optimization": "Clipping gradients during backpropagation.",
          "impact": "Prevents exploding gradients and stabilizes training for the deep DQN architecture."
        },
        {
          "optimization": "Target network updates at regular intervals.",
          "impact": "Reduces the problem of having constantly shifting Q-value targets and leads to more stable learning."
        }
      ],
      "lessons_learned": [
        "The importance of careful reward shaping and valid-action checks in puzzle-style RL environments.",
        "Managing exploration schedules (epsilon decay) is critical to ensure sufficient exploration while converging to better policies.",
        "Periodic evaluation and checkpointing helps track progress and prevent catastrophic model regression."
      ]
    },
    "files": [
      {
        "path": "main.py",
        "description": "Entry point for running the Tkinter-based UI, evaluating or training the AI, and launching the main 2048 game window.",
        "functions": [
          {
            "name": "evaluate_dummy",
            "description": "Evaluates the DummyAI over a fixed number of episodes and prints average and std of scores.",
            "parameters": [],
            "returns": {
              "type": "None",
              "description": "Prints the average score and standard deviation."
            }
          },
          {
            "name": "train_dqn_with_ui",
            "description": "Spawns a Tkinter window with the default environment and GUI setup to potentially train the DQN from the UI.",
            "parameters": [],
            "returns": {
              "type": "None",
              "description": "Opens a Tkinter window and starts the UI main loop."
            }
          },
          {
            "name": "eval_dqn",
            "description": "Loads a DQN model, evaluates it for 100 episodes, and prints statistics (avg, max, min, std).",
            "parameters": [],
            "returns": {
              "type": "None",
              "description": "Prints evaluation metrics for the loaded model."
            }
          },
          {
            "name": "play_game",
            "description": "Lets a human user play 2048 with a Tkinter UI (no AI).",
            "parameters": [],
            "returns": {
              "type": "None",
              "description": "Starts the GUI loop for human play."
            }
          },
          {
            "name": "main",
            "description": "Default function called when main.py is executed; launches train_dqn_with_ui() by default.",
            "parameters": [],
            "returns": {
              "type": "None",
              "description": "Starts the 2048 UI or other functionality as uncommented."
            }
          }
        ],
        "classes": [],
        "imports": [
          {
            "module": "tkinter",
            "purpose": "Used for creating the graphical user interface."
          },
          {
            "module": "math",
            "purpose": "Used for mathematical operations (e.g., log, exponential)."
          },
          {
            "module": "game.gameui",
            "purpose": "Imports GUI components (Game2048GUI, TrainingWindow, etc.)."
          },
          {
            "module": "game.game_engine",
            "purpose": "Imports GameEngine for the 2048 logic and GameRecorder for recording games."
          },
          {
            "module": "agent.dummy.ai",
            "purpose": "Imports a baseline dummy AI for testing and evaluation."
          },
          {
            "module": "agent.dqn.ai",
            "purpose": "Imports the DQN-based AI for 2048."
          }
        ]
      },
      {
        "path": "requirements.txt",
        "description": "Lists Python dependencies needed to run the project (e.g., numpy, torch, matplotlib, tqdm).",
        "functions": [],
        "classes": [],
        "imports": []
      },
      {
        "path": "todo",
        "description": "A simple text file capturing possible future improvements and ideas (e.g., recursion, attention, animations).",
        "functions": [],
        "classes": [],
        "imports": []
      },
      {
        "path": "agent/dqn/ai.py",
        "description": "Implements the DQN agent for 2048. Includes the Dueling DQN architecture, replay memory, training methods, and evaluation routines.",
        "functions": [
          {
            "name": "state_to_tensor",
            "description": "Converts a 4x4 grid of integers into a log2 scale tensor for PyTorch.",
            "parameters": [
              {
                "name": "grid",
                "type": "list[list[int]]",
                "description": "4x4 matrix representing the 2048 board."
              }
            ],
            "returns": {
              "type": "torch.Tensor",
              "description": "Shape (1,1,4,4) containing log2 cell values."
            }
          },
          {
            "name": "boltzmann_exploration",
            "description": "Selects an action index via Boltzmann (softmax) exploration given Q-values and a temperature.",
            "parameters": [
              {
                "name": "q_values",
                "type": "torch.Tensor",
                "description": "Q-values for each possible action."
              },
              {
                "name": "temperature",
                "type": "float",
                "description": "Temperature parameter controlling exploration. Defaults to 1.0."
              }
            ],
            "returns": {
              "type": "int",
              "description": "The selected action index."
            }
          }
        ],
        "classes": [
          {
            "name": "DQN",
            "description": "A Dueling DQN neural network for 2048. Splits into a value and advantage stream, then combines them.",
            "methods": [
              {
                "name": "forward",
                "description": "Computes Q-values for a given batch of states.",
                "parameters": [
                  {
                    "name": "x",
                    "type": "torch.Tensor",
                    "description": "A batch of states of shape (batch_size, 1, 4, 4)."
                  }
                ],
                "returns": {
                  "type": "torch.Tensor",
                  "description": "Q-values for each action, shape (batch_size, 4)."
                }
              }
            ],
            "attributes": [
              {
                "name": "feature",
                "type": "nn.Sequential",
                "description": "Shared feature extractor layers (fully connected + layer norms + ReLU)."
              },
              {
                "name": "value_stream",
                "type": "nn.Sequential",
                "description": "Outputs a single scalar as the state-value."
              },
              {
                "name": "advantage_stream",
                "type": "nn.Sequential",
                "description": "Outputs the advantage values for each possible action."
              }
            ]
          },
          {
            "name": "ReplayMemory",
            "description": "A simple replay buffer storing tuples of (state, action, reward, next_state, done).",
            "methods": [
              {
                "name": "push",
                "description": "Stores a transition in the replay buffer.",
                "parameters": [
                  {
                    "name": "state",
                    "type": "torch.Tensor",
                    "description": "Current state."
                  },
                  {
                    "name": "action",
                    "type": "int",
                    "description": "Chosen action index."
                  },
                  {
                    "name": "reward",
                    "type": "float",
                    "description": "Reward received after taking the action."
                  },
                  {
                    "name": "next_state",
                    "type": "torch.Tensor",
                    "description": "Subsequent state after the action."
                  },
                  {
                    "name": "done",
                    "type": "bool",
                    "description": "Whether the episode has terminated."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Updates the memory buffer."
                }
              },
              {
                "name": "sample",
                "description": "Randomly samples a batch of transitions.",
                "parameters": [
                  {
                    "name": "batch_size",
                    "type": "int",
                    "description": "Number of transitions to sample."
                  }
                ],
                "returns": {
                  "type": "(tuple of lists)",
                  "description": "Returns a tuple of (state, action, reward, next_state, done) lists."
                }
              }
            ],
            "attributes": [
              {
                "name": "memory",
                "type": "collections.deque",
                "description": "Deque buffer holding transitions up to capacity."
              }
            ]
          },
          {
            "name": "DQN2048",
            "description": "A class wrapping a DQN network for the 2048 game, including training loops, action selection, and evaluation.",
            "methods": [
              {
                "name": "__init__",
                "description": "Initializes the DQN2048 class, sets up the networks, optionally loads model weights, and prepares the device.",
                "parameters": [
                  {
                    "name": "model_path",
                    "type": "str or None",
                    "description": "Path to a saved model for loading, or None to start fresh."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Constructs the DQN2048 agent with policy and target networks."
                }
              },
              {
                "name": "create_random_policy",
                "description": "Saves initial random weights to a checkpoint file.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Writes an initial random model state_dict to disk."
                }
              },
              {
                "name": "predict_move",
                "description": "Given a game state and valid actions, selects the action with highest Q-value (or random fallback if none valid).",
                "parameters": [
                  {
                    "name": "state",
                    "type": "list[list[int]]",
                    "description": "The current 4x4 board state."
                  },
                  {
                    "name": "valid_actions",
                    "type": "list of str",
                    "description": "A list of valid actions (e.g. ['up', 'down'])."
                  }
                ],
                "returns": {
                  "type": "str",
                  "description": "One of the valid actions, e.g. 'up' or 'down'."
                }
              },
              {
                "name": "select_action",
                "description": "Implements an epsilon-greedy (plus optional Boltzmann) exploration policy.",
                "parameters": [
                  {
                    "name": "state",
                    "type": "torch.Tensor",
                    "description": "Current game state as a tensor."
                  },
                  {
                    "name": "env",
                    "type": "GameEngine",
                    "description": "Game environment for checking valid actions."
                  },
                  {
                    "name": "steps_done",
                    "type": "int",
                    "description": "Global step count used in epsilon decay scheduling."
                  },
                  {
                    "name": "eps_start",
                    "type": "float",
                    "description": "Initial epsilon for exploration."
                  },
                  {
                    "name": "eps_end",
                    "type": "float",
                    "description": "Minimum epsilon."
                  },
                  {
                    "name": "eps_decay",
                    "type": "float",
                    "description": "Decay parameter controlling how epsilon changes over steps."
                  },
                  {
                    "name": "temperature",
                    "type": "float",
                    "description": "Temperature parameter for Boltzmann exploration."
                  }
                ],
                "returns": {
                  "type": "str",
                  "description": "Chosen action from ['up','down','left','right']. Defaults to random valid if constraints fail."
                }
              },
              {
                "name": "train_in_thread",
                "description": "Background training method that runs episodes in a loop, collects experiences, updates networks, and saves checkpoints. Communicates progress via a queue.",
                "parameters": [
                  {
                    "name": "env",
                    "type": "GameEngine",
                    "description": "The 2048 environment."
                  },
                  {
                    "name": "episodes",
                    "type": "int",
                    "description": "Number of training episodes."
                  },
                  {
                    "name": "gamma",
                    "type": "float",
                    "description": "Discount factor for future rewards."
                  },
                  {
                    "name": "lr_start",
                    "type": "float",
                    "description": "Starting learning rate."
                  },
                  {
                    "name": "lr_end",
                    "type": "float",
                    "description": "Ending learning rate (if a schedule is used)."
                  },
                  {
                    "name": "batch_size",
                    "type": "int",
                    "description": "Size of each replay sample batch."
                  },
                  {
                    "name": "memory_size",
                    "type": "int",
                    "description": "Capacity of the replay memory."
                  },
                  {
                    "name": "target_update",
                    "type": "int",
                    "description": "Interval (in steps) to update target network."
                  },
                  {
                    "name": "eps_start",
                    "type": "float",
                    "description": "Initial epsilon value."
                  },
                  {
                    "name": "eps_end",
                    "type": "float",
                    "description": "Minimum epsilon value."
                  },
                  {
                    "name": "eps_decay",
                    "type": "float",
                    "description": "Exponential decay rate for epsilon."
                  },
                  {
                    "name": "temperature",
                    "type": "float",
                    "description": "Temperature for Boltzmann exploration."
                  },
                  {
                    "name": "result_queue",
                    "type": "queue.Queue",
                    "description": "Queue to send progress updates (episode, score, etc.) to the main thread."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Runs training until all episodes complete, then saves a final checkpoint."
                }
              },
              {
                "name": "train",
                "description": "A simpler, synchronous training loop using tqdm for progress. Periodically evaluates and logs results.",
                "parameters": [
                  {
                    "name": "env",
                    "type": "GameEngine",
                    "description": "2048 environment instance."
                  },
                  {
                    "name": "episodes",
                    "type": "int",
                    "description": "Number of episodes to train."
                  },
                  {
                    "name": "eval_interval",
                    "type": "int",
                    "description": "Frequency (episodes) at which to run evaluation."
                  },
                  {
                    "name": "eval_episodes",
                    "type": "int",
                    "description": "Number of episodes for each evaluation pass."
                  },
                  {
                    "name": "recorder",
                    "type": "GameRecorder or None",
                    "description": "Optional recorder for saving game data."
                  }
                ],
                "returns": {
                  "type": "list of tuples",
                  "description": "Evaluation results: (episode, average_score, std) for each evaluation interval."
                }
              },
              {
                "name": "optimize_model",
                "description": "Performs one optimization step on a batch from replay memory. Implements Double DQN updates.",
                "parameters": [
                  {
                    "name": "memory",
                    "type": "ReplayMemory",
                    "description": "Replay buffer containing past experiences."
                  },
                  {
                    "name": "gamma",
                    "type": "float",
                    "description": "Discount factor."
                  },
                  {
                    "name": "optimizer",
                    "type": "torch.optim.Optimizer",
                    "description": "Optimizer used for gradient descent."
                  },
                  {
                    "name": "batch_size",
                    "type": "int",
                    "description": "Number of transitions in a batch."
                  }
                ],
                "returns": {
                  "type": "float",
                  "description": "The loss value for logging and analysis."
                }
              },
              {
                "name": "eval",
                "description": "Runs evaluation episodes with no exploration, optionally records the best game.",
                "parameters": [
                  {
                    "name": "env",
                    "type": "GameEngine",
                    "description": "2048 environment for evaluation."
                  },
                  {
                    "name": "n_episodes",
                    "type": "int",
                    "description": "Number of episodes to run for evaluation."
                  },
                  {
                    "name": "recorder",
                    "type": "GameRecorder or None",
                    "description": "Optional game recorder."
                  },
                  {
                    "name": "episode",
                    "type": "int or None",
                    "description": "Training episode index (used for naming saved recordings)."
                  }
                ],
                "returns": {
                  "type": "(float, float, float, float)",
                  "description": "Average score, max score, min score, and score std across evaluation episodes."
                }
              }
            ],
            "attributes": [
              {
                "name": "net",
                "type": "DQN",
                "description": "The main policy network."
              },
              {
                "name": "target_net",
                "type": "DQN",
                "description": "Target network for more stable Q-learning updates."
              },
              {
                "name": "device",
                "type": "torch.device",
                "description": "Indicates whether the agent is running on CPU or GPU."
              },
              {
                "name": "action_map",
                "type": "dict",
                "description": "Maps action indices (0..3) to strings ('up','down','left','right')."
              }
            ]
          }
        ],
        "imports": [
          {
            "module": "math",
            "purpose": "Logarithms for converting cell values to log2 representation."
          },
          {
            "module": "random",
            "purpose": "Random action selection and replay sampling."
          },
          {
            "module": "numpy",
            "purpose": "Array manipulations for the board and states."
          },
          {
            "module": "torch",
            "purpose": "Deep learning framework for building and training neural networks."
          },
          {
            "module": "time",
            "purpose": "Measuring training steps and performance."
          },
          {
            "module": "queue",
            "purpose": "Communication between training thread and UI thread."
          },
          {
            "module": "threading",
            "purpose": "Background training thread execution."
          },
          {
            "module": "torch.nn",
            "purpose": "Neural network layers for the DQN architecture."
          },
          {
            "module": "torch.optim",
            "purpose": "Optimizers (e.g., Adam) for training the network."
          },
          {
            "module": "tqdm",
            "purpose": "Progress bar during synchronous training loops."
          },
          {
            "module": "collections.deque",
            "purpose": "Efficient queue structure for replay memory."
          }
        ]
      },
      {
        "path": "agent/dummy/ai.py",
        "description": "Implements a simple baseline (DummyAI) that picks random moves among all possible directions.",
        "functions": [],
        "classes": [
          {
            "name": "DummyAI",
            "description": "A naive AI agent that randomly selects among ['left', 'right', 'up', 'down'].",
            "methods": [
              {
                "name": "predict_move",
                "description": "Returns one of the four directions randomly (ignoring current state).",
                "parameters": [
                  {
                    "name": "state",
                    "type": "list[list[int]]",
                    "description": "The current 4x4 board state (not actually used by this AI)."
                  }
                ],
                "returns": {
                  "type": "str",
                  "description": "One of ['left','right','up','down'] randomly."
                }
              },
              {
                "name": "eval",
                "description": "Evaluates DummyAI over multiple games, optionally recording a subset of them.",
                "parameters": [
                  {
                    "name": "env",
                    "type": "GameEngine",
                    "description": "The 2048 game environment."
                  },
                  {
                    "name": "n_episodes",
                    "type": "int",
                    "description": "Number of episodes to play."
                  },
                  {
                    "name": "recorder",
                    "type": "GameRecorder or None",
                    "description": "Optional recorder for logging game steps."
                  }
                ],
                "returns": {
                  "type": "(float, float)",
                  "description": "Average score and standard deviation across episodes."
                }
              }
            ],
            "attributes": []
          }
        ],
        "imports": [
          {
            "module": "random",
            "purpose": "Randomly choose moves and randomly pick episodes to record."
          },
          {
            "module": "numpy",
            "purpose": "Compute mean and std of scores."
          },
          {
            "module": "tqdm",
            "purpose": "Progress bar for evaluation loop."
          }
        ]
      },
      {
        "path": "game/game_engine.py",
        "description": "Contains the core 2048 game logic (GameEngine) and a GameRecorder for capturing move sequences.",
        "functions": [],
        "classes": [
          {
            "name": "GameEngine",
            "description": "Maintains the 4x4 grid, handles merges and movement, spawns new tiles, and checks for game termination.",
            "methods": [
              {
                "name": "__init__",
                "description": "Initializes a fresh 4x4 grid, zero score, and resets the game.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Creates internal structures for the game state."
                }
              },
              {
                "name": "reset",
                "description": "Clears the board, spawns two tiles, and resets the score and done flag.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Prepares a new 2048 game from scratch."
                }
              },
              {
                "name": "step",
                "description": "Takes a move action (up/down/left/right), updates the board, spawns a new tile, and checks if the game is done.",
                "parameters": [
                  {
                    "name": "action",
                    "type": "str",
                    "description": "One of ['up','down','left','right']."
                  }
                ],
                "returns": {
                  "type": "(list[list[int]], int, bool, dict)",
                  "description": "New grid, reward (score difference), done flag, and extra info."
                }
              },
              {
                "name": "is_valid_action",
                "description": "Checks if a given move would change the board state.",
                "parameters": [
                  {
                    "name": "action",
                    "type": "str",
                    "description": "Move direction to simulate."
                  }
                ],
                "returns": {
                  "type": "bool",
                  "description": "True if the move alters the grid, False otherwise."
                }
              },
              {
                "name": "get_state",
                "description": "Returns a copy of the current 4x4 grid.",
                "parameters": [],
                "returns": {
                  "type": "list[list[int]]",
                  "description": "Deep copy of the current board."
                }
              },
              {
                "name": "print_grid",
                "description": "Prints the current grid to stdout for debugging.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Displays the grid rows in the console."
                }
              },
              {
                "name": "get_score",
                "description": "Returns the current game score.",
                "parameters": [],
                "returns": {
                  "type": "int",
                  "description": "The total points accumulated."
                }
              },
              {
                "name": "is_done",
                "description": "Checks whether the game is over (no moves left).",
                "parameters": [],
                "returns": {
                  "type": "bool",
                  "description": "True if no moves remain."
                }
              }
            ],
            "attributes": [
              {
                "name": "grid",
                "type": "list[list[int]]",
                "description": "4x4 board representing tile values."
              },
              {
                "name": "score",
                "type": "int",
                "description": "Current game score."
              },
              {
                "name": "done",
                "type": "bool",
                "description": "Indicates if the game is over."
              }
            ]
          },
          {
            "name": "GameRecorder",
            "description": "Records each step of a game (state, action, next_state, reward, done, score) for future replay or analysis.",
            "methods": [
              {
                "name": "__init__",
                "description": "Initializes an empty recording buffer.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Sets up internal structures for storing steps."
                }
              },
              {
                "name": "reset",
                "description": "Clears the current recording data.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Resets the recorder to an empty state."
                }
              },
              {
                "name": "start",
                "description": "Begins recording a new game session, clearing previous data.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Sets recorder.active = True and empties the buffer."
                }
              },
              {
                "name": "record_step",
                "description": "Appends a single game step to the buffer if active.",
                "parameters": [
                  {
                    "name": "state",
                    "type": "list[list[int]]",
                    "description": "Board state before the move."
                  },
                  {
                    "name": "action",
                    "type": "str",
                    "description": "Move direction, e.g. 'left'."
                  },
                  {
                    "name": "next_state",
                    "type": "list[list[int]]",
                    "description": "Resulting board after the move."
                  },
                  {
                    "name": "reward",
                    "type": "int",
                    "description": "Points gained by merging tiles."
                  },
                  {
                    "name": "done",
                    "type": "bool",
                    "description": "Whether the game ended after this move."
                  },
                  {
                    "name": "score",
                    "type": "int",
                    "description": "The total score after this move."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Adds the step data to the recording list."
                }
              },
              {
                "name": "stop",
                "description": "Ends the recording session.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Sets recorder.active = False."
                }
              },
              {
                "name": "save_to_json",
                "description": "Saves the recorded data to a JSON file.",
                "parameters": [
                  {
                    "name": "filename",
                    "type": "str",
                    "description": "Where to save the JSON data (default='game_record.json')."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Writes the current recording to disk."
                }
              },
              {
                "name": "load_from_json",
                "description": "Loads a recording from a JSON file and returns it.",
                "parameters": [
                  {
                    "name": "filename",
                    "type": "str",
                    "description": "Path to the JSON file containing recorded moves."
                  }
                ],
                "returns": {
                  "type": "list",
                  "description": "List of steps recorded in that file."
                }
              }
            ],
            "attributes": [
              {
                "name": "recording",
                "type": "list",
                "description": "Holds the step-by-step data of the game if active."
              },
              {
                "name": "active",
                "type": "bool",
                "description": "True if currently recording game steps."
              }
            ]
          }
        ],
        "imports": [
          {
            "module": "random",
            "purpose": "Random tile placement and random position selection."
          },
          {
            "module": "json",
            "purpose": "Reading/writing recorded game data in JSON format."
          }
        ]
      },
      {
        "path": "game/gameui.py",
        "description": "Provides a Tkinter-based GUI (Game2048GUI) to play or train the DQN agent. Also defines windows for training configuration and progress (TrainingWindow, TrainingConfigWindow).",
        "functions": [],
        "classes": [
          {
            "name": "Game2048GUI",
            "description": "Main Tkinter GUI for playing 2048, managing AI or manual moves, loading recordings, and controlling replay.",
            "methods": [
              {
                "name": "__init__",
                "description": "Configures the Tkinter window and internal state for the 2048 GUI, sets up layout, event bindings, etc.",
                "parameters": [
                  {
                    "name": "master",
                    "type": "tk.Tk",
                    "description": "The root Tkinter window."
                  },
                  {
                    "name": "engine",
                    "type": "GameEngine",
                    "description": "The 2048 game engine instance."
                  },
                  {
                    "name": "ai_model",
                    "type": "DQN2048 or None",
                    "description": "Optional AI model for automatically choosing moves."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Initializes the UI, draws the board, binds controls."
                }
              },
              {
                "name": "load_model",
                "description": "Opens a file dialog to load a .pth model file and initializes DQN2048 with those weights.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Updates self.ai_model and enables AI-based moves if successful."
                }
              },
              {
                "name": "ai_move",
                "description": "Uses the loaded DQN model to pick a valid next move and applies it to the game engine.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Updates the board state with the AI-chosen action."
                }
              },
              {
                "name": "_do_move",
                "description": "Applies a given action to the game engine, records it if active, and updates the UI.",
                "parameters": [
                  {
                    "name": "action",
                    "type": "str",
                    "description": "The move direction (left/right/up/down)."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Changes the engine state, draws updated tiles, and checks for game over."
                }
              },
              {
                "name": "open_training_config",
                "description": "Opens a pop-up window (TrainingConfigWindow) to configure training hyperparameters.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Displays the training config UI."
                }
              },
              {
                "name": "draw_tiles",
                "description": "Redraws the entire 4x4 board on the canvas according to the current engine state.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Erases previous tiles and redraws all cells, then updates score labels."
                }
              },
              {
                "name": "toggle_recording",
                "description": "Starts or stops recording the current game. If stopping, prompts user to save the recording as JSON.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Toggles self.is_recording and updates UI."
                }
              },
              {
                "name": "load_game",
                "description": "Loads a recorded game from JSON, sets the UI to replay mode, and preps the replay controls.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Populates self.loaded_data and transitions to replay UI."
                }
              },
              {
                "name": "new_game",
                "description": "Starts a fresh 2048 game, optionally canceling replay mode if active.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Resets the engine and UI for a new round."
                }
              },
              {
                "name": "human_move",
                "description": "Handles arrow key input from the user to move tiles if not in replay mode.",
                "parameters": [
                  {
                    "name": "action",
                    "type": "str",
                    "description": "One of the four directions triggered by arrow key."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Applies the action if valid and updates the board."
                }
              }
            ],
            "attributes": []
          },
          {
            "name": "TrainingWindow",
            "description": "A Tkinter Toplevel that manages a background training thread and displays real-time training metrics via matplotlib plots.",
            "methods": [
              {
                "name": "__init__",
                "description": "Creates the training UI, sets up labels and plots, then spawns a background thread to run the DQN training loop.",
                "parameters": [
                  {
                    "name": "parent",
                    "type": "tk.Tk",
                    "description": "The parent Tkinter window."
                  },
                  {
                    "name": "ai",
                    "type": "DQN2048",
                    "description": "The RL agent to train."
                  },
                  {
                    "name": "env",
                    "type": "GameEngine",
                    "description": "The 2048 environment."
                  },
                  {
                    "name": "episodes",
                    "type": "int",
                    "description": "Number of training episodes."
                  },
                  {
                    "name": "gamma",
                    "type": "float",
                    "description": "Discount factor."
                  },
                  {
                    "name": "lr_start",
                    "type": "float",
                    "description": "Initial learning rate."
                  },
                  {
                    "name": "lr_end",
                    "type": "float",
                    "description": "Final learning rate."
                  },
                  {
                    "name": "batch_size",
                    "type": "int",
                    "description": "Batch size for replay samples."
                  },
                  {
                    "name": "memory_size",
                    "type": "int",
                    "description": "Replay buffer capacity."
                  },
                  {
                    "name": "target_update",
                    "type": "int",
                    "description": "Steps between target network updates."
                  },
                  {
                    "name": "eps_start",
                    "type": "float",
                    "description": "Initial epsilon for exploration."
                  },
                  {
                    "name": "eps_end",
                    "type": "float",
                    "description": "Minimum epsilon value."
                  },
                  {
                    "name": "eps_decay",
                    "type": "float",
                    "description": "Decay factor for epsilon."
                  },
                  {
                    "name": "temperature",
                    "type": "float",
                    "description": "Temperature for Boltzmann exploration."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Displays the training UI and starts polling for progress updates."
                }
              }
            ],
            "attributes": []
          },
          {
            "name": "TrainingConfigWindow",
            "description": "A configuration window allowing the user to set hyperparameters for DQN training, then launch the TrainingWindow.",
            "methods": [
              {
                "name": "__init__",
                "description": "Builds text fields for hyperparameters, plus a 'Start Training' button that opens a TrainingWindow.",
                "parameters": [
                  {
                    "name": "parent",
                    "type": "tk.Tk",
                    "description": "Parent Tkinter window."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Presents user interface to enter hyperparameters for training."
                }
              }
            ],
            "attributes": []
          }
        ],
        "imports": [
          {
            "module": "tkinter",
            "purpose": "Used heavily for GUI components: windows, buttons, labels, dialogs."
          },
          {
            "module": "matplotlib",
            "purpose": "Plots training metrics in real time."
          },
          {
            "module": "agent.dqn.ai",
            "purpose": "Imports the DQN2048 class for training or AI moves."
          },
          {
            "module": "game.game_engine",
            "purpose": "GameEngine and GameRecorder for interacting with and recording the 2048 environment."
          },
          {
            "module": "time",
            "purpose": "Delays in replay mode or animation, measuring training times."
          },
          {
            "module": "threading",
            "purpose": "Runs the DQN training loop in a background thread."
          },
          {
            "module": "queue",
            "purpose": "Communication channel between training thread and the UI."
          }
        ]
      },
      {
        "path": "other/game_animation.py",
        "description": "Contains an alternative Tkinter-based 2048 game class with tile-slide animations and minimal AI. Demonstrates transitions and merges visually.",
        "functions": [],
        "classes": [
          {
            "name": "Game2048",
            "description": "Another GUI/class for 2048 featuring animations on tile moves or merges.",
            "methods": [
              {
                "name": "__init__",
                "description": "Sets up the game board and draws an animated grid. Binds arrow keys for tile movement.",
                "parameters": [
                  {
                    "name": "master",
                    "type": "tk.Tk",
                    "description": "The root Tkinter window."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Creates a game state and draws the initial 4x4 grid."
                }
              },
              {
                "name": "restart_game",
                "description": "Resets the grid, score, and best score for a fresh 2048 session.",
                "parameters": [],
                "returns": {
                  "type": "None",
                  "description": "Clears the board and spawns two new tiles."
                }
              },
              {
                "name": "move_left",
                "description": "Moves tiles left with animation steps, merges them, then spawns a new tile.",
                "parameters": [
                  {
                    "name": "event",
                    "type": "tk.Event",
                    "description": "Triggered by left arrow key."
                  }
                ],
                "returns": {
                  "type": "None",
                  "description": "Updates the grid, calls the animation method, checks for game over."
                }
              }
            ],
            "attributes": [
              {
                "name": "grid",
                "type": "list[list[int]]",
                "description": "Holds the tile values."
              },
              {
                "name": "score",
                "type": "int",
                "description": "Current game score."
              },
              {
                "name": "best_score",
                "type": "int",
                "description": "Tracks the highest score achieved so far."
              },
              {
                "name": "animation_in_progress",
                "type": "bool",
                "description": "Prevents overlapping animations when True."
              }
            ]
          }
        ],
        "imports": [
          {
            "module": "tkinter",
            "purpose": "Used to create the GUI window, handle events, and draw animations."
          },
          {
            "module": "random",
            "purpose": "Places new tiles randomly and merges them with some probability."
          },
          {
            "module": "tkinter.messagebox",
            "purpose": "Displays pop-up messages (e.g., game over or tile creation)."
          }
        ]
      },
      {
        "path": "utils/gpu_available.py",
        "description": "Prints whether CUDA (GPU) or CPU is being used by PyTorch. Helps confirm if GPU acceleration is available.",
        "functions": [],
        "classes": [],
        "imports": [
          {
            "module": "torch",
            "purpose": "Checks torch.cuda.is_available() and sets the device accordingly."
          }
        ]
      }
    ]
  }
}
