### Directory Structure ###
Directory structure:
└── francoishup-2048/
    ├── main.py
    ├── requirements.txt
    ├── todo
    ├── agent/
    │   ├── dqn/
    │   │   ├── ai.py
    │   │   ├── model_chkpt/
    │   │   │   └── dqn_2048_37750.pth
    │   │   └── recorded_games/
    │   │       └── evaluation_game_37750_5464.json
    │   └── dummy/
    │       └── ai.py
    ├── game/
    │   ├── game_engine.py
    │   ├── gameui.py
    │   └── save_games/
    │       └── my_2048_game.json
    ├── other/
    │   └── game_animation.py
    └── utils/
        └── gpu_available.py


### Code Content ###
================================================
File: main.py
================================================
import tkinter as tk
import math
from game.gameui import Game2048GUI, TrainingWindow, TrainingConfigWindow
from game.game_engine import GameEngine, GameRecorder
from agent.dummy.ai import DummyAI
from agent.dqn.ai import DQN2048

def evaluate_dummy():
    engine = GameEngine()
    ai = DummyAI()  

     # Evaluate the AI
    recorder = GameRecorder()
    avg_score, score_std = ai.eval(engine,100, recorder) # recorder
    print(avg_score, score_std)

# def train_dqn():
#     engine = GameEngine()
#     ai = DQN2048() # "agent\dqn\model_chkpt\dqn_2048.pth"  
#     recorder = GameRecorder()

#     # Train the AI
#     ai.train(engine, episodes=100000, recorder=recorder)

def train_dqn_with_ui():
    root = tk.Tk()
    root.title("2048 Main")

    # Possibly show the normal 2048 game UI
    env = GameEngine()
    game_gui = Game2048GUI(root, engine=env, ai_model=None)

    root.mainloop()


def eval_dqn():
    engine = GameEngine()
    ai = DQN2048("agent/dqn/model_chkpt/dqn_2048_best.pth")

    # Evaluate the AI
    recorder = GameRecorder()
    avg_score, max_score, min_score, score_std = ai.eval(engine, 100, recorder) # recorder
    print(f"avg_score={avg_score:.2f}, max_score={max_score:.2f}, min_score={min_score:.2f}, std={score_std:.2f}")

def play_game():
    root = tk.Tk()
    engine = GameEngine()

    # Inference with the Game hook
    game_gui = Game2048GUI(root, engine)
    root.mainloop()

def main():
    train_dqn_with_ui()
    # train_dqn()
    # eval_dqn()
    # play_game()
    # evaluate_dummy()

if __name__ == "__main__":
    main()


================================================
File: requirements.txt
================================================
numpy
tqdm
torch
matplotlib

================================================
File: todo
================================================
[?] recursive, ttt?
[?] use attention and recusivity on state of the game (for a simple state) instead of cnn and pixel?  

[frozen] Add animation of sliding tiles. 
[DONE] Add tiles to inifinit (2048,4096,...,128k,...)
[DONE] Be able to Record games:  
        -Keep games states at a given moment so it could replay it infront of me;
        -User should be able to record it's game (every actions and states) to train the ai in the future. 

================================================
File: agent/dqn/ai.py
================================================
import math
import random
import numpy as np
import torch
import time
import queue
import threading
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from collections import deque


# =============================================================================
#                               1. DQN NETWORK
# =============================================================================
# class DQN(nn.Module):
#     """
#     A simpler MLP network for 2048.
#     The board is 4x4, so we flatten it into 16 inputs.
#     Output: Q-values for 4 possible actions (up, down, left, right).
#     """

#     def __init__(self):
#         super().__init__()

#         self.fc = nn.Sequential(
#             nn.Linear(16, 512),
#             nn.LayerNorm(512),
#             nn.ReLU(),

#             nn.Linear(512, 512),
#             nn.LayerNorm(512),
#             nn.ReLU(),

#             nn.Linear(512, 512),
#             nn.LayerNorm(512),
#             nn.ReLU(),

#             nn.Linear(512, 256),
#             nn.LayerNorm(256),
#             nn.ReLU(),

#             nn.Linear(256, 4)
#         )

#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         """
#         x shape: (batch_size, 1, 4, 4)
#         Flatten to: (batch_size, 16)
#         Returns Q-values for each action: shape (batch_size, 4).
#         """
#         x = x.view(x.size(0), -1)
#         return self.fc(x)

class DQN(nn.Module):
    """
    A large Dueling DQN network for 2048.
    1) We flatten the 4x4 grid (16 inputs).
    2) Pass through big hidden layers (e.g., 1024 neurons each).
    3) Split into two heads:
       - Value stream (outputs 1 value for the state)
       - Advantage stream (outputs 4 values for each action)
    4) Q-values = Value + (Advantage - mean(Advantage))
    """

    def __init__(self):
        super().__init__()

        # Common feature extractor
        # 16 -> 1024 -> 1024 -> 1024 -> feature
        self.feature = nn.Sequential(
            nn.Linear(16, 1024),
            nn.LayerNorm(1024),
            nn.ReLU(),

            nn.Linear(1024, 1024),
            nn.LayerNorm(1024),
            nn.ReLU(),

            nn.Linear(1024, 1024),
            nn.LayerNorm(1024),
            nn.ReLU(),
        )

        # Value stream: produces a single scalar value for the state
        self.value_stream = nn.Sequential(
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )

        # Advantage stream: produces an advantage for each action (4)
        self.advantage_stream = nn.Sequential(
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Linear(512, 4)
        )

    def forward(self, x):
        """
        x shape: (batch_size, 1, 4, 4)
        Flatten to: (batch_size, 16)
        Returns Q-values for each action: shape (batch_size, 4).
        """
        # Flatten
        x = x.view(x.size(0), -1)

        # Extract features
        features = self.feature(x)  # shape: (batch_size, 1024)

        # Compute value and advantage
        value = self.value_stream(features)          # shape: (batch_size, 1)
        advantage = self.advantage_stream(features)  # shape: (batch_size, 4)

        # Combine into Q-values
        # Q(s,a) = V(s) + [ A(s,a) - mean(A(s,a) over actions ) ]
        mean_advantage = advantage.mean(dim=1, keepdim=True)
        q_values = value + (advantage - mean_advantage)

        return q_values




# =============================================================================
#                     2. UTILITY FUNCTIONS & REPLAY MEMORY
# =============================================================================
def state_to_tensor(grid):
    """
    Convert the 4x4 grid to log2 scale (0 if cell=0, else log2(cell_value)).
    Output shape: (1, 1, 4, 4) as a PyTorch tensor.
    """
    mat = []
    for row in grid:
        row_vals = [math.log2(val) if val > 0 else 0 for val in row]
        mat.append(row_vals)
    mat = np.array(mat, dtype=np.float32)  # shape (4, 4)
    mat = np.expand_dims(mat, axis=(0, 1)) # shape (1, 1, 4, 4)
    return torch.from_numpy(mat)


def boltzmann_exploration(q_values, temperature=1.0):
    """
    Boltzmann (softmax) exploration: sample an action index
    with probability ~ exp(Q / temperature).
    """
    probabilities = torch.softmax(q_values / temperature, dim=0).cpu().numpy()
    return np.random.choice(len(q_values), p=probabilities)


class ReplayMemory:
    """
    Simple replay buffer to store (state, action, reward, next_state, done).
    """

    def __init__(self, capacity: int):
        self.memory = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int):
        batch = random.sample(self.memory, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return state, action, reward, next_state, done

    def __len__(self):
        return len(self.memory)


# =============================================================================
#                     3. MAIN DQN 2048 AGENT CLASS
# =============================================================================
class DQN2048:
    """
    A Deep Q-Network (DQN) based agent for the 2048 game.
    Includes:
      - a policy network (self.net)
      - a target network (self.target_net)
      - training loops (train, train_in_thread)
      - evaluation and inference methods
    """

    def __init__(self, model_path=None):
        """
        - model_path: path to a saved model (if any). If None, create a random policy.
        - self.net: the main policy network
        - self.target_net: the fixed target network
        """
        # Initialize policy network
        self.net = DQN()

        # Load weights if specified; otherwise create a random policy checkpoint
        if model_path:
            self.net.load_state_dict(torch.load(model_path))
        else:
            self.create_random_policy()

        # Device setup
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.net.to(self.device)

        # Initialize target network
        self.target_net = DQN().to(self.device)
        self.target_net.load_state_dict(self.net.state_dict())
        self.target_net.eval()

        # Action map: 0->'up', 1->'down', 2->'left', 3->'right'
        self.action_map = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}

        print(f"Using device: {self.device}")

    # -------------------------------------------------------------------------
    #                          CREATION / INFERENCE
    # -------------------------------------------------------------------------
    def create_random_policy(self):
        """
        Optionally save initial random weights to a file.
        """
        torch.save(self.net.state_dict(), "agent/dqn/model_chkpt/dqn_2048_init.pth")

    def predict_move(self, state, valid_actions):
        """
        Predict the next move for a given state, ensuring it is valid.
        If no valid action is found, picks a random valid action.
        :param state: current 4x4 game state (list of lists)
        :param valid_actions: list of valid actions (e.g., ['up', 'down'])
        :return: chosen action (e.g., 'up')
        """
        state_tensor = state_to_tensor(state).to(self.device)
        with torch.no_grad():
            q_values = self.net(state_tensor).squeeze(0).cpu().numpy()

        # Sort all possible actions by Q-value (descending)
        sorted_actions = sorted(
            self.action_map.items(),
            key=lambda x: q_values[x[0]],
            reverse=True
        )

        # Pick the first valid action from the sorted list
        for action_idx, action_name in sorted_actions:
            if action_name in valid_actions:
                return action_name

        # Fallback: random valid action if none matched
        if valid_actions:
            return random.choice(valid_actions)
        raise ValueError("No valid actions available")

    # -------------------------------------------------------------------------
    #                       ACTION SELECTION (EPS-GREEDY)
    # -------------------------------------------------------------------------
    def select_action(
        self,
        state,
        env,
        steps_done,
        eps_start=1.0,
        eps_end=0.1,
        eps_decay=100000,
        temperature=1.0
    ):
        """
        Epsilon-greedy with optional Boltzmann exploration.
        :param state: Torch tensor of shape (1,1,4,4)
        :param env: GameEngine environment
        :param steps_done: how many steps have been taken
        :param eps_start, eps_end, eps_decay: epsilon scheduling
        :param temperature: for Boltzmann exploration
        :return: chosen action (str: 'up', 'down', 'left', or 'right')
        """
        eps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1.0 * steps_done / eps_decay)

        # Exploration
        if random.random() < eps_threshold:
            # Boltzmann exploration
            with torch.no_grad():
                q_values = self.net(state).squeeze(0)  # shape: (4,)
                action_idx = boltzmann_exploration(q_values, temperature=temperature)
                chosen_action = self.action_map[action_idx]
            # Validate action
            if env.is_valid_action(chosen_action):
                return chosen_action
            valid_actions = [a for a in self.action_map.values() if env.is_valid_action(a)]
            return random.choice(valid_actions) if valid_actions else random.choice(self.action_map.values())

        # Exploitation
        else:
            with torch.no_grad():
                q_values = self.net(state).squeeze(0).cpu().numpy()
                action_idx = np.argmax(q_values)
                chosen_action = self.action_map[action_idx]
            # Validate action
            if env.is_valid_action(chosen_action):
                return chosen_action
            valid_actions = [a for a in self.action_map.values() if env.is_valid_action(a)]
            return random.choice(valid_actions) if valid_actions else random.choice(self.action_map.values())

    # -------------------------------------------------------------------------
    #                       BACKGROUND TRAINING THREAD
    # -------------------------------------------------------------------------
    def train_in_thread(
        self,
        env,
        episodes,
        gamma,
        lr_start,
        lr_end,
        batch_size,
        memory_size,
        target_update,
        eps_start,
        eps_end,
        eps_decay,
        temperature,
        result_queue
    ):
        """
        Trains the DQN in a background thread, sending progress updates to `result_queue`.
        Now includes periodic and best-score checkpoint saving.
        """
        from agent.dqn.ai import ReplayMemory

        steps_done = 0
        highest_tile_overall = 0
        best_score = -float('inf')   # Track best game score

        # Set up replay memory and optimizer
        memory = ReplayMemory(memory_size)
        current_lr = lr_start
        optimizer = optim.Adam(self.net.parameters(), lr=current_lr)

        for episode in range(1, episodes + 1):
            start_time = time.time()

            self.net.train()
            env.reset()
            state = state_to_tensor(env.get_state()).to(self.device)

            total_reward = 0.0
            steps_this_episode = 0
            done = False
            episode_loss = 0.0
            loss_count = 0
            highest_tile_ep = 0

            while not done:
                # Epsilon schedule
                epsilon = eps_end + (eps_start - eps_end) * math.exp(-1.0 * steps_done / eps_decay)

                action = self.select_action(
                    state,
                    env,
                    steps_done,
                    eps_start=eps_start,
                    eps_end=eps_end,
                    eps_decay=eps_decay,
                    temperature=temperature
                )
                steps_done += 1
                steps_this_episode += 1

                next_state_grid, score_diff, done, _ = env.step(action)

                # Track highest tile
                flat_grid = [val for row in next_state_grid for val in row]
                highest_tile_local = max(flat_grid)
                if highest_tile_local > highest_tile_ep:
                    highest_tile_ep = highest_tile_local

                # Basic reward shaping
                reward = score_diff

                next_state = state_to_tensor(next_state_grid).to(self.device)
                action_idx = list(self.action_map.values()).index(action)
                memory.push(state, action_idx, reward, next_state, done)

                state = next_state
                total_reward += reward

                # Optimize if enough samples
                if len(memory) >= batch_size:
                    loss_val = self.optimize_model(memory, gamma, optimizer, batch_size)
                    episode_loss += loss_val
                    loss_count += 1

                # Update target net periodically
                if steps_done % target_update == 0:
                    self.target_net.load_state_dict(self.net.state_dict())

            # Update overall highest tile
            if highest_tile_ep > highest_tile_overall:
                highest_tile_overall = highest_tile_ep

            # Steps per second
            end_time = time.time()
            elapsed = max(end_time - start_time, 1e-6)
            steps_per_sec = steps_this_episode / elapsed

            avg_loss = episode_loss / (loss_count if loss_count > 0 else 1)
            episode_score = env.get_score()

            # 1) Check if we have a new best score
            if episode_score > best_score:
                best_score = episode_score
                # Save a "best" checkpoint
                torch.save(self.net.state_dict(), "agent/dqn/model_chkpt/dqn_2048_best.pth")
                print(f"[Checkpoint] New best score={best_score:.0f} at Episode {episode}, model saved!")

            # 2) Periodically save a checkpoint (e.g. every 500 episodes)
            if episode % 500 == 0:
                ckpt_path = f"agent/dqn/model_chkpt/dqn_2048_{episode}.pth"
                torch.save(self.net.state_dict(), ckpt_path)
                print(f"[Checkpoint] Episode {episode}, model saved to {ckpt_path}")

            # Send progress to the main thread via queue
            msg = {
                "episode": episode,
                "episodes_total": episodes,
                "steps_done": steps_done,
                "steps_per_sec": steps_per_sec,
                "highest_tile_overall": highest_tile_overall,
                "epsilon": epsilon,
                "reward": total_reward,
                "loss": avg_loss,
                "score": episode_score,
            }
            result_queue.put(msg)

        # At the end of all training
        torch.save(self.net.state_dict(), "agent/dqn/model_chkpt/dqn_2048_final.pth")
        print("[Checkpoint] Final model saved!")
        result_queue.put(None)

    # -------------------------------------------------------------------------
    #                    TRADITIONAL CLI TRAINING LOOP
    # -------------------------------------------------------------------------
    def train(
        self,
        env,
        episodes=5000,
        eval_interval=250,
        eval_episodes=50,
        recorder=None
    ):
        """
        Main training loop using a synchronous approach + tqdm progress bar.
        :param env: 2048 environment
        :param episodes: number of training episodes
        :param eval_interval: how often to evaluate
        :param eval_episodes: number of evaluation episodes
        :param recorder: optional GameRecorder
        :return: list of evaluation results
        """
        gamma = 0.99
        lr = 5e-4
        batch_size = 128
        memory_size = 100000
        target_update = 25
        steps_done = 0

        memory = ReplayMemory(memory_size)
        optimizer = optim.Adam(self.net.parameters(), lr=lr)

        eval_results = []
        highest_tile = 0
        target_tile = 2048
        total_spaces = 16
        best_average_score = 0

        with tqdm(range(episodes), desc="Training Progress", unit="episode") as pbar:
            for episode in pbar:
                self.net.train()
                env.reset()
                state = state_to_tensor(env.get_state()).to(self.device)
                total_reward = 0

                while not env.is_done():
                    action = self.select_action(state, env, steps_done)
                    steps_done += 1

                    next_state_grid, score_diff, done, _ = env.step(action)

                    next_state_grid_array = np.array(next_state_grid).flatten()
                    highest_tile_number = np.max(next_state_grid_array)
                    empty_spaces = sum(1 for x in next_state_grid if x == 0)
                    normalized_highest_tile = highest_tile_number / target_tile

                    # Reward shaping
                    reward = score_diff - 0.01
                    reward += normalized_highest_tile * 10
                    reward += (empty_spaces / total_spaces) * 5

                    if done:
                        penalty = -50 - (target_tile - highest_tile_number) * 0.05
                        reward += penalty

                    next_state = state_to_tensor(next_state_grid).to(self.device)
                    action_idx = list(self.action_map.values()).index(action)
                    memory.push(state, action_idx, reward, next_state, done)

                    state = next_state
                    total_reward += reward

                    if len(memory) >= batch_size:
                        self.optimize_model(memory, gamma, optimizer, batch_size)

                    if steps_done % target_update == 0:
                        self.target_net.load_state_dict(self.net.state_dict())

                if highest_tile < highest_tile_number:
                    highest_tile = highest_tile_number

                # Update progress bar
                pbar.set_postfix({
                    "Reward": f"{total_reward:.2f}",
                    "Highest tile": f"{highest_tile:.0f}",
                    "Steps": steps_done,
                })

                # Periodic evaluation
                if episode % eval_interval == 0 and episode > 0:
                    avg_score, max_score, min_score, score_std = self.eval(env, eval_episodes, recorder, episode)
                    eval_results.append((episode, avg_score, score_std))
                    tqdm.write(
                        f"[Eval at Episode {episode}] "
                        f"avg_score={avg_score:.2f}, max_score={max_score:.2f}, "
                        f"min_score={min_score:.2f}, std={score_std:.2f}"
                    )
                    with open("agent/dqn/logs/training_log.csv", "a") as log_file:
                        log_file.write(f"{episode},{avg_score},{max_score},{min_score},{score_std}\n")

                    # Save best model by average score
                    if best_average_score < avg_score:
                        best_average_score = avg_score
                        torch.save(self.net.state_dict(), "agent/dqn/model_chkpt/dqn_2048_best.pth")
                    torch.save(self.net.state_dict(), f"agent/dqn/model_chkpt/dqn_2048_{episode}.pth")

        torch.save(self.net.state_dict(), "agent/dqn/model_chkpt/dqn_2048_final.pth")
        return eval_results

    # -------------------------------------------------------------------------
    #                       SINGLE TRAINING STEP (OPTIMIZE)
    # -------------------------------------------------------------------------
    def optimize_model(self, memory, gamma, optimizer, batch_size):
        """
        Sample from replay memory and run a single optimization step.
        Uses a Double DQN approach for the target update.
        Returns loss.item() for logging.
        """
        batch_state, batch_action, batch_reward, batch_next_state, batch_done = memory.sample(batch_size)

        device = self.device
        batch_state = torch.cat(batch_state).to(device)
        batch_next_state = torch.cat(batch_next_state).to(device)
        batch_action = torch.tensor(batch_action, dtype=torch.long, device=device)
        batch_reward = torch.tensor(batch_reward, dtype=torch.float32, device=device)
        batch_done = torch.tensor(batch_done, dtype=torch.bool, device=device)

        # Current Q values
        q_values = self.net(batch_state)  # shape: (batch_size, 4)
        q_values = q_values.gather(1, batch_action.unsqueeze(1)).squeeze(1)  # shape: (batch_size,)

        # Next Q values (Double DQN)
        with torch.no_grad():
            best_actions = self.net(batch_next_state).argmax(dim=1, keepdim=True)
            target_q = self.target_net(batch_next_state).gather(1, best_actions).squeeze(1)
            target_q[batch_done] = 0.0

        target_value = batch_reward + gamma * target_q

        # SmoothL1 (Huber) loss
        loss = nn.SmoothL1Loss()(q_values, target_value)

        # Gradient clipping & optimization
        torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=1)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        return loss.item()

    # -------------------------------------------------------------------------
    #                               EVALUATION
    # -------------------------------------------------------------------------
    def eval(self, env, n_episodes=10, recorder=None, episode=None):
        """
        Evaluate the model over n_episodes with no exploration.
        If recorder is provided, record the best game.
        """
        self.net.eval()
        self.net.to(self.device)

        best_score = float('-inf')
        best_game = None
        scores = []

        for _ in tqdm(range(n_episodes), desc="Evaluation Progress", leave=False):
            env.reset()
            total_score = 0

            if recorder is not None:
                recorder.reset()
                recorder.start()

            while not env.is_done():
                valid_actions = [
                    a for a in self.action_map.values()
                    if env.is_valid_action(a)
                ]
                if not valid_actions:
                    print("No valid actions left, ending game.")
                    break

                with torch.no_grad():
                    state = state_to_tensor(env.get_state()).to(self.device)
                    q_vals = self.net(state).squeeze(0).cpu().numpy()
                    action_idx = np.argmax(q_vals)
                    action = self.action_map[action_idx]

                    # Fallback if invalid
                    if action not in valid_actions:
                        action = random.choice(valid_actions)

                old_state = env.get_state()
                new_grid, reward, done, _ = env.step(action)
                total_score += reward

                if recorder is not None and recorder.active:
                    recorder.record_step(
                        state=old_state,
                        action=action,
                        next_state=new_grid,
                        reward=reward,
                        done=done,
                        score=env.get_score()
                    )

            scores.append(total_score)
            if total_score > best_score:
                best_score = total_score
                best_game = list(recorder.recording) if recorder else None

            if recorder is not None:
                recorder.stop()

        # Optionally save the best game
        if recorder is not None and best_game is not None:
            recorder.recording = best_game
            filename = (
                f"agent/dqn/recorded_games/evaluation_game_{episode}_{best_score}.json"
                if episode is not None
                else f"agent/dqn/recorded_games/evaluation_game_{best_score}.json"
            )
            recorder.save_to_json(filename)

        avg_score = np.mean(scores)
        max_score = np.max(scores)
        min_score = np.min(scores)
        score_std = np.std(scores)

        self.net.train()  # back to training mode
        return avg_score, max_score, min_score, score_std


================================================
File: agent/dummy/ai.py
================================================
import random
import numpy as np
from tqdm import tqdm

class DummyAI:
    """Example: picks random moves from ['left','right','up','down']"""
    
    def predict_move(self, state):
        # For this dummy AI, we ignore 'state' and pick a random action
        return random.choice(['left', 'right', 'up', 'down'])
    
    def eval(self, env, n_episodes=10, recorder=None):
        """
        Evaluate the model in a deterministic environment.
        
        :param env: The environment (GameEngine).
        :param n_episodes: Number of games to run.
        :param recorder: A GameRecorder object, or None (no recording).
        """
        print(f"Evaluating {n_episodes} games")

        # 1) Randomly pick up to 3 episodes to record (if recorder is provided)
        if recorder is not None:
            num_to_record = min(3, n_episodes)
            episodes_to_record = set(random.sample(range(n_episodes), num_to_record))
        else:
            episodes_to_record = set()

        scores = []

        # 2) Loop over the number of episodes
        for episode_index in tqdm(range(n_episodes), desc="Evaluation Progress", leave=False):
            env.reset()

            # If this episode is one we plan to record
            if episode_index in episodes_to_record and recorder is not None:
                recorder.reset()
                recorder.start()

            # 3) Play until the game is done
            while not env.is_done():
                # Capture old state
                old_state = env.get_state()
                
                # Choose an action (dummy random)
                action = self.predict_move(old_state)
                
                # Environment step
                new_grid, reward, done, _ = env.step(action)
                
                # If we're recording, store this transition
                if recorder is not None and recorder.active:
                    recorder.record_step(
                        state=old_state,
                        action=action,
                        next_state=new_grid,
                        reward=reward,
                        done=done,
                        score=env.score  # or env.get_score()
                    )

            # The game is finished
            scores.append(env.score)

            # 4) If we were recording this episode, stop and save
            if episode_index in episodes_to_record and recorder is not None and recorder.active:
                recorder.stop()
                # Save each recorded episode to a unique file
                recorder.save_to_json(f"evaluation_game_{episode_index}.json")

        # 5) Compute average score and standard deviation over the episodes
        avg_score = np.mean(scores)
        score_std = np.std(scores)
        return avg_score, score_std

================================================
File: game/game_engine.py
================================================
import random
import json

GRID_SIZE = 4

class GameEngine:
    def __init__(self):
        self.grid = [[0]*GRID_SIZE for _ in range(GRID_SIZE)]
        self.score = 0
        self.done = False  # Will be True when no more moves are possible
        self.reset()

    def reset(self):
        """Reset the board to start a new game."""
        self.grid = [[0]*GRID_SIZE for _ in range(GRID_SIZE)]
        self.score = 0
        self.done = False
        self._add_random_tile()
        self._add_random_tile()

    def step(self, action):
        """
        Perform a move in the environment.
        action is one of ['up','down','left','right'] or an integer representing that move.
        Returns: (new_grid, reward, done, info)
        - new_grid: the state after the move
        - reward: how much score was gained from merges this move
        - done: True if no moves left (game over)
        - info: optional debug info
        """
        if self.done:
            return self.grid, 0, True, {}

        old_score = self.score
        old_grid = [row[:] for row in self.grid]

        # 1. Apply the move
        self._move(action)

        # If the board didn’t change, no new tile and no reward
        if self.grid == old_grid:
            return self.grid, 0, self.done, {}

        # 2. Add a random tile
        self._add_random_tile()

        # 3. Check game over
        self.done = self._check_game_over()

        score_diff = self.score - old_score
        return self.grid, score_diff, self.done, {}

    def _add_random_tile(self):
        """Add a random tile of value 2 or 4 to an empty cell."""
        empty_cells = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE) if self.grid[i][j] == 0]
        if not empty_cells:
            return
        i, j = random.choice(empty_cells)
        self.grid[i][j] = 4 if random.random() < 0.1 else 2

    def _move(self, direction):
        """Apply move logic: up/down/left/right."""
        if direction == 'left':
            self.grid = self._slide_left(self.grid)
        elif direction == 'right':
            reversed_grid = [row[::-1] for row in self.grid]
            new_reversed = self._slide_left(reversed_grid)
            self.grid = [row[::-1] for row in new_reversed]
        elif direction == 'up':
            transposed = self._transpose(self.grid)
            moved = self._slide_left(transposed)
            self.grid = self._transpose(moved)
        elif direction == 'down':
            transposed = self._transpose(self.grid)
            reversed_grid = [row[::-1] for row in transposed]
            new_reversed = self._slide_left(reversed_grid)
            unreversed = [row[::-1] for row in new_reversed]
            self.grid = self._transpose(unreversed)

    def _slide_left(self, grid):
        """Slide everything left in a 2D grid and return the new grid."""
        new_grid = []
        for row in grid:
            compressed = self._compress(row)
            merged = self._merge(compressed)
            final = self._compress(merged)
            new_grid.append(final)
        return new_grid

    def _compress(self, row):
        """Push non-zero values to the front (left) of the row."""
        new_row = [num for num in row if num != 0]
        new_row += [0] * (GRID_SIZE - len(new_row))
        return new_row

    def _merge(self, row):
        """Merge adjacent equal tiles from left to right."""
        for i in range(GRID_SIZE - 1):
            if row[i] != 0 and row[i] == row[i+1]:
                row[i] *= 2
                self.score += row[i]  # add to the total score
                row[i+1] = 0
        return row

    def _transpose(self, grid):
        return [list(r) for r in zip(*grid)]

    def _check_game_over(self):
        """Check if there are no moves left."""
        # If there's an empty cell, not over
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                if self.grid[i][j] == 0:
                    return False
                # check horizontal neighbor
                if j < GRID_SIZE - 1 and self.grid[i][j] == self.grid[i][j+1]:
                    return False
                # check vertical neighbor
                if i < GRID_SIZE - 1 and self.grid[i][j] == self.grid[i+1][j]:
                    return False
        return True
    
    def is_valid_action(self, action):
        """Check if an action is valid (leads to a state change)."""
        original_grid = [row[:] for row in self.grid]

        # Simulate the move
        self._move(action)

        # Compare grids
        is_valid = self.grid != original_grid

        # Restore the original state
        self.grid = original_grid

        return is_valid

    def get_state(self):
        """Return a copy of the current grid (useful for AI)."""
        return [row[:] for row in self.grid]
    
    def print_grid(self):
        for row in self.grid:
            print(row) 

    def get_score(self):
        return self.score

    def is_done(self):
        return self.done

class GameRecorder:
    def __init__(self):
        self.reset()

    def reset(self):
        """
        Clear the current recording buffer.
        """
        self.recording = []
        self.active = False

    def start(self):
        """
        Start recording moves.
        """
        self.reset()
        self.active = True

    def record_step(self, state, action, next_state, reward, done, score):
        """
        Record a single step: the old state, the action,
        the resulting next_state, the reward, whether done, and score.
        """
        if not self.active:
            return
        self.recording.append({
            "state": state,         # 4x4 grid before the move
            "action": action,       # e.g., 'left', 'right', etc.
            "next_state": next_state,
            "reward": reward,
            "done": done,
            "score": score
        })

    def stop(self):
        """
        Stop recording.
        """
        self.active = False

    def save_to_json(self, filename="game_record.json"):
        """
        Save the current recording as JSON.
        """
        with open(filename, "w") as f:
            json.dump(self.recording, f)
        # print(f"Game recording saved to {filename}")

    def load_from_json(self, filename="game_record.json"):
        """
        Load a recorded game from JSON file.
        Returns the loaded recording (list of steps).
        """
        with open(filename, "r") as f:
            data = json.load(f)
        return data

================================================
File: game/gameui.py
================================================
import tkinter as tk
from tkinter import messagebox, filedialog
from game.game_engine import GameEngine, GameRecorder
import math

import tkinter as tk
import threading
import queue
import time

import matplotlib
matplotlib.use("TkAgg")
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.figure import Figure

from agent.dqn.ai import DQN2048

    
GRID_SIZE = 4
CELL_SIZE = 100
CELL_PADDING = 10
BACKGROUND_COLOR = "#92877d"
EMPTY_CELL_COLOR = "#9e948a"
# The original dictionary for base tiles up to 2048
CELL_COLORS = {
    2: "#eee4da",
    4: "#ede0c8",
    8: "#f2b179",
    16: "#f59563",
    32: "#f67c5f",
    64: "#f65e3b",
    128: "#edcf72",
    256: "#edcc61",
    512: "#edc850",
    1024: "#edc53f",
    2048: "#edc22e",
}

# A small list of extra colors we can cycle through for tiles above 2048.
# You can add as many as you want or define a color gradient.
EXTRA_COLORS = [
    "#edc22e",  # the same as 2048 for 4096
    "#bdc22e",
    "#9dc22e",
    "#7dc22e",
    "#5dc22e",
    "#3dc22e",
    "#1dc22e",
    "#1dc25e",
    "#1dc27e",
    "#1dc29e",
    "#1dc2be",
]
FONT = ("Verdana", 24, "bold")

# Dictionary to map the textual action to an arrow symbol
ACTION_SYMBOLS = {
    "left": "←",
    "right": "→",
    "up": "↑",
    "down": "↓"
}

def get_tile_color(value):
    """
    Return a color for any 2^N tile.
    - If it's <= 2048, use the base color from CELL_COLORS.
    - If it's > 2048, generate or pick an extended color.
    """
    if value in CELL_COLORS:
        return CELL_COLORS[value]
    else:
        # For anything above 2048, pick a color from EXTRA_COLORS
        # based on the tile's exponent
        # e.g., 4096 -> exponent = 12, 8192 -> 13, etc.
        exponent = int(math.log2(value))
        # We'll cycle through EXTRA_COLORS so we don't run out
        # shift by 11 because 2^11 = 2048
        index = (exponent - 11) % len(EXTRA_COLORS)
        return EXTRA_COLORS[index]
    
class Game2048GUI:
    def __init__(self, master, engine, ai_model=None):
        self.master = master
        self.engine = engine
        self.ai_model = ai_model
        self.animation_in_progress = False

        self.best_score = 0
        self.is_recording = False
        self.recorder = GameRecorder()

        self.loaded_data = []
        self.current_step = -1
        self.is_replaying = False
        self.is_playing = False

        master.title("2048 Game")
        master.resizable(False, False)

        # ------------------- TOP FRAMES LAYOUT -------------------
        top_frame = tk.Frame(master)
        top_frame.pack(pady=5)

        # 1) score_frame -> Score + Best
        self.score_frame = tk.Frame(top_frame)
        self.score_frame.pack(side=tk.LEFT, padx=10)

        # 2) button_frame -> New Game, Load Game, Record, AI Move
        self.button_frame = tk.Frame(top_frame)
        self.button_frame.pack(side=tk.LEFT, padx=10)

        # 3) replay_frame -> Back, Next, Play/Pause/Replay, Speed slider, Action label
        self.replay_frame = tk.Frame(top_frame)
        self.replay_frame.pack_forget()

        # ------------------- SCORE FRAME -------------------
        self.score_label = tk.Label(self.score_frame, text="Score: 0", font=("Verdana", 16), width=10, anchor="w")
        self.score_label.pack(anchor='w')

        self.best_score_label = tk.Label(self.score_frame, text=f"Best: {self.best_score}", font=("Verdana", 16), width=10, anchor="w")
        self.best_score_label.pack(anchor='w')

        # ------------------- BUTTON FRAME -------------------
        self.new_game_button = tk.Button(
            self.button_frame, text="New Game", font=("Verdana", 12),
            command=self.new_game
        )
        self.new_game_button.pack(side=tk.LEFT, padx=5)

        self.load_button = tk.Button(
            self.button_frame, text="Load Game", font=("Verdana", 12),
            command=self.load_game
        )
        self.load_button.pack(side=tk.LEFT, padx=5)

        self.record_button = tk.Button(
            self.button_frame, text="Record", font=("Verdana", 12),
            command=self.toggle_recording
        )
        self.record_button.pack(side=tk.LEFT, padx=5)

        self.load_model_button = tk.Button(
            self.button_frame, text="Load Model", font=("Verdana", 12),
            command=self.load_model
        )
        self.load_model_button.pack(side=tk.LEFT, padx=5)

        self.ai_button = tk.Button(
            self.button_frame, text="AI Move", font=("Verdana", 12),
            command=self.ai_move,
            state=tk.DISABLED  # Initially disabled
        )
        self.ai_button.pack(side=tk.LEFT, padx=5) 

        self.train_button = tk.Button(
            self.button_frame, text="Train AI", font=("Verdana", 12),
            command=self.open_training_config
        )
        self.train_button.pack(side=tk.LEFT, padx=5)

        # ------------------- REPLAY FRAME -------------------
        self.back_button = tk.Button(
            self.replay_frame, text="Back", font=("Verdana", 12),
            command=self.back_step
        )
        self.back_button.pack(side=tk.LEFT, padx=5)

        self.next_button = tk.Button(
            self.replay_frame, text="Next", font=("Verdana", 12),
            command=self.next_step
        )
        self.next_button.pack(side=tk.LEFT, padx=5)

        self.play_button = tk.Button(
            self.replay_frame, text="Play", font=("Verdana", 12),
            command=self.toggle_play_pause
        )
        self.play_button.pack(side=tk.LEFT, padx=5)

        self.play_speed_scale = tk.Scale(
            self.replay_frame,
            from_=1000,
            to=10,
            resolution=10,
            orient='horizontal',
            label='Speed (ms)'
        )
        self.play_speed_scale.set(500)
        self.play_speed_scale.pack(side=tk.LEFT, padx=5)

        # Label to display the current action as an arrow
        self.action_label = tk.Label(self.replay_frame, text="Action: N/A", font=("Verdana", 12), width=10, anchor="w")
        self.action_label.pack(side=tk.LEFT, padx=10)

        # ------------------- CANVAS -------------------
        canvas_width = GRID_SIZE * (CELL_SIZE + CELL_PADDING) + CELL_PADDING
        canvas_height = GRID_SIZE * (CELL_SIZE + CELL_PADDING) + CELL_PADDING
        self.canvas = tk.Canvas(master, width=canvas_width, height=canvas_height, bg=BACKGROUND_COLOR)
        self.canvas.pack()

        # Key bindings
        master.bind("<Up>", self.on_up)
        master.bind("<Down>", self.on_down)
        master.bind("<Left>", self.on_left)
        master.bind("<Right>", self.on_right)

        # Initial draw
        self.draw_tiles()
    
    # -------------------------------------------------------------------------
    #                            LOAD MODEL
    # -------------------------------------------------------------------------
    def load_model(self):
        """
        Open a file dialog to select a model, load it, and enable the AI Move button.
        """
        file_path = filedialog.askopenfilename(
            title="Load AI Model",
            filetypes=[("Model Files", "*.pth"), ("All Files", "*.*")]
        )

        if not file_path:
            messagebox.showinfo("Load Model", "No model file selected.")
            return

        try:
            # Load the model
            self.ai_model = DQN2048(model_path=file_path)
            self.ai_button.config(state=tk.NORMAL)  # Enable the AI Move button
            messagebox.showinfo("Load Model", f"Model loaded successfully from:\n{file_path}")
        except Exception as e:
            messagebox.showerror("Load Model", f"Failed to load model:\n{e}")

    # -------------------------------------------------------------------------
    #                             AI MOVE
    # -------------------------------------------------------------------------
    def ai_move(self):
        """
        Use the loaded AI model to make a move on behalf of the user.
        Ensures the AI doesn't get stuck by predicting invalid moves.
        """
        if not self.ai_model:
            messagebox.showerror("AI Move", "No AI model loaded.")
            return

        if self.engine.is_done():
            messagebox.showinfo("Game Over", "The game is over. Start a new game.")
            return

        # Get valid actions
        valid_actions = [
            action for action in ['up', 'down', 'left', 'right']
            if self.engine.is_valid_action(action)
        ]

        if not valid_actions:
            messagebox.showinfo("No Moves", "No valid moves left. Game over.")
            return

        # Use the AI model to predict the next move
        try:
            action = self.ai_model.predict_move(self.engine.get_state(), valid_actions)
            self._do_move(action)
        except ValueError as e:
            messagebox.showerror("AI Error", f"AI could not make a valid move: {e}")

    # -------------------------------------------------------------------------
    #                            DO MOVE
    # -------------------------------------------------------------------------
    def _do_move(self, action):
        if self.animation_in_progress:
            return

        old_state = self.engine.get_state()
        new_grid, reward, done, info = self.engine.step(action)
        new_score = self.engine.get_score()

        self.draw_tiles()

        # If we're recording, store this step
        if self.recorder.active:
            self.recorder.record_step(
                old_state,
                action,
                new_grid,
                reward,
                done,
                new_score
            )

        if done:
            if new_score > self.best_score:
                self.best_score = new_score
                self.best_score_label.config(text=f"Best: {self.best_score}")
            messagebox.showinfo("2048", f"Game Over!\nYour Score: {new_score}")

    # -------------------------------------------------------------------------
    #                             TRAIN AI
    # -------------------------------------------------------------------------
    def open_training_config(self):
        """
        Open the TrainingConfigWindow to set training parameters.
        """
        TrainingConfigWindow(self.master)

    # -------------------------------------------------------------------------
    #                             DRAWING
    # -------------------------------------------------------------------------
    def draw_tiles(self):
        self.canvas.delete("all")
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                x0 = CELL_PADDING + j * (CELL_SIZE + CELL_PADDING)
                y0 = CELL_PADDING + i * (CELL_SIZE + CELL_PADDING)
                x1 = x0 + CELL_SIZE
                y1 = y0 + CELL_SIZE
                self.canvas.create_rectangle(x0, y0, x1, y1, fill=EMPTY_CELL_COLOR, outline="")

        grid = self.engine.get_state()
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                value = grid[i][j]
                if value != 0:
                    self.draw_single_tile(i, j, value)

        current_score = self.engine.get_score()
        self.score_label.config(text=f"Score: {current_score}")
        if current_score > self.best_score:
            self.best_score = current_score
        self.best_score_label.config(text=f"Best: {self.best_score}")

        self.master.update_idletasks()

    def draw_single_tile(self, i, j, value):
        x0 = CELL_PADDING + j * (CELL_SIZE + CELL_PADDING)
        y0 = CELL_PADDING + i * (CELL_SIZE + CELL_PADDING)
        x1 = x0 + CELL_SIZE
        y1 = y0 + CELL_SIZE

        color = get_tile_color(value) 
        self.canvas.create_rectangle(x0, y0, x1, y1, fill=color, outline="")
        x_center = x0 + CELL_SIZE / 2
        y_center = y0 + CELL_SIZE / 2
        self.canvas.create_text(x_center, y_center, text=str(value), font=FONT, fill="#776e65")

    # -------------------------------------------------------------------------
    #                         RECORD / STOP
    # -------------------------------------------------------------------------
    def toggle_recording(self):
        if not self.is_recording:
            self.recorder.start()
            self.is_recording = True
            self.record_button.config(text="Stop")
            print("Recording started.")
        else:
            self.recorder.stop()
            self.is_recording = False
            print("Recording stopped.")

            file_path = filedialog.asksaveasfilename(
                defaultextension=".json",
                filetypes=[("JSON Files", "*.json")],
                initialfile="my_2048_game.json",
                title="Save 2048 Recording"
            )
            if file_path:
                self.recorder.save_to_json(file_path)
                print(f"Recording saved to: {file_path}")

            self.record_button.config(text="Record")

    # -------------------------------------------------------------------------
    #                   LOAD A PREVIOUS GAME (REPLAY MODE)
    # -------------------------------------------------------------------------
    def load_game(self):
        if not self.engine.is_done() and not self.is_replaying:
            answer = messagebox.askyesno(
                "Abandon game?",
                "A game is currently in progress. Are you sure you want to load another game and lose this progress?"
            )
            if not answer:
                return

        file_path = filedialog.askopenfilename(
            defaultextension=".json",
            filetypes=[("JSON Files", "*.json")],
            title="Open 2048 Recording"
        )
        if not file_path:
            return

        data = self.recorder.load_from_json(file_path)
        if not data:
            messagebox.showerror("Error", "No valid data found in file.")
            return

        self.loaded_data = data
        self.current_step = -1
        self.is_replaying = True
        self.is_playing = False

        # Set the board to the first step
        first_step = self.loaded_data[0]
        self.engine.grid = [row[:] for row in first_step["state"]]
        self.engine.score = first_step["score"]
        self.engine.done = False
        self.draw_tiles()

        # Hide AI and Record buttons
        self.ai_button.pack_forget()
        self.record_button.pack_forget()

        # Show replay controls
        self.replay_frame.pack(side=tk.LEFT, padx=10)

        # Reset the Play button
        self.play_button.config(text="Play")
        # Reset action label
        self.action_label.config(text="Action: N/A")

    # -------------------------------------------------------------------------
    #                  REPLAY CONTROLS: BACK, NEXT, PLAY/PAUSE, REPLAY
    # -------------------------------------------------------------------------
    def back_step(self):
        """Go one step back in self.loaded_data, if possible."""
        if not self.is_replaying or len(self.loaded_data) == 0:
            return

        if self.current_step <= 0:
            messagebox.showinfo("Replay", "Already at the earliest step.")
            return

        self.current_step -= 1
        step_data = self.loaded_data[self.current_step]
        # Move board to next_state
        next_grid = step_data["next_state"]
        self.engine.grid = [row[:] for row in next_grid]
        self.engine.score = step_data["score"]
        self.engine.done = step_data["done"]
        self.draw_tiles()

        # Convert the textual action to an arrow symbol
        action_text = step_data["action"]
        arrow = ACTION_SYMBOLS.get(action_text, action_text)
        self.action_label.config(text=f"Action: {arrow}")

    def next_step(self):
        """Go one step forward in self.loaded_data, showing 'next_state'."""
        if not self.is_replaying or len(self.loaded_data) == 0:
            return

        if self.current_step >= len(self.loaded_data) - 1:
            messagebox.showinfo("Replay finished", "Reached the end of the recorded game.")
            self.play_button.config(text="Replay")
            self.is_playing = False
            return

        self.current_step += 1
        step_data = self.loaded_data[self.current_step]
        next_grid = step_data["next_state"]
        self.engine.grid = [row[:] for row in next_grid]
        self.engine.score = step_data["score"]
        self.engine.done = step_data["done"]
        self.draw_tiles()

        # Convert textual action to arrow
        action_text = step_data["action"]
        arrow = ACTION_SYMBOLS.get(action_text, action_text)
        self.action_label.config(text=f"Action: {arrow}")

        if self.current_step >= len(self.loaded_data) - 1:
            messagebox.showinfo("Replay finished", "Reached the end of the recorded game.")
            self.play_button.config(text="Replay")
            self.is_playing = False

    def toggle_play_pause(self):
        """Button can say "Play", "Pause", or "Replay"."""
        if not self.is_replaying:
            return

        current_text = self.play_button.cget("text")

        if current_text == "Play":
            self.is_playing = True
            self.play_button.config(text="Pause")
            self.auto_play()

        elif current_text == "Pause":
            self.is_playing = False
            self.play_button.config(text="Play")

        elif current_text == "Replay":
            self.current_step = -1
            self.is_playing = False

            if self.loaded_data:
                first_step = self.loaded_data[0]
                self.engine.grid = [row[:] for row in first_step["state"]]
                self.engine.score = first_step["score"]
                self.engine.done = False
                self.draw_tiles()
            self.play_button.config(text="Play")
            self.action_label.config(text="Action: N/A")

    def auto_play(self):
        """Automatically move forward every X ms until the end or paused."""
        if not self.is_replaying or not self.is_playing:
            return

        self.next_step()  # step forward

        if self.current_step < len(self.loaded_data) - 1 and self.is_playing:
            delay = self.play_speed_scale.get()
            self.master.after(delay, self.auto_play)
        else:
            # Reached end or paused
            self.is_playing = False

    # -------------------------------------------------------------------------
    #                             NEW GAME
    # -------------------------------------------------------------------------
    def new_game(self):
        if self.is_replaying:
            answer = messagebox.askyesno(
                "Quit Replay?",
                "You are currently replaying a recorded game. Quit replay and start a new game?"
            )
            if not answer:
                return

            self.is_replaying = False
            self.is_playing = False
            self.loaded_data = []
            self.current_step = -1

            # Hide replay controls
            self.replay_frame.pack_forget()

            # Show AI and Record again
            self.record_button.pack(side=tk.LEFT, padx=5)
            self.ai_button.pack(side=tk.LEFT, padx=5)

        self.engine.reset()
        self.draw_tiles()

    # -------------------------------------------------------------------------
    #                        KEYBOARD EVENTS
    # -------------------------------------------------------------------------
    def on_left(self, event):
        self.human_move('left')

    def on_right(self, event):
        self.human_move('right')

    def on_up(self, event):
        self.human_move('up')

    def on_down(self, event):
        self.human_move('down')

    def human_move(self, action):
        if self.is_replaying:
            messagebox.showinfo("Replay mode", "Currently in replay mode. Use 'Back', 'Next', or 'Play/Pause/Replay'.")
            return
        if self.engine.is_done():
            return
        self._do_move(action)


class TrainingWindow(tk.Toplevel):
    """
    A window that runs training in a background thread and displays
    real-time metrics (Reward, Loss, Score, etc.) using matplotlib.
    This is similar to what we've built before, but it now accepts
    all hyperparameters as arguments.
    """
    def __init__(
        self,
        parent,
        ai,
        env,
        episodes=5000,
        gamma=0.99,
        lr_start=5e-4,
        lr_end=5e-5,
        batch_size=128,
        memory_size=100000,
        target_update=25,
        eps_start=1.0,
        eps_end=0.1,
        eps_decay=40000,
        temperature=1.0,
    ):
        super().__init__(parent)
        self.title("2048 Training")
        self.geometry("800x600")
        self.resizable(False, False)

        self.ai = ai
        self.env = env

        # Store the hyperparameters
        self.episodes = episodes
        self.gamma = gamma
        self.lr_start = lr_start
        self.lr_end = lr_end
        self.batch_size = batch_size
        self.memory_size = memory_size
        self.target_update = target_update
        self.eps_start = eps_start
        self.eps_end = eps_end
        self.eps_decay = eps_decay
        self.temperature = temperature

        # We’ll create a queue for receiving training status
        self.result_queue = queue.Queue()

        # -- Top Frame with Info Labels --
        top_frame = tk.Frame(self)
        top_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)

        self.label_episode = tk.Label(top_frame, text="Episode: 0 / 0", width=18, anchor="w")
        self.label_episode.pack(side=tk.LEFT, padx=10)

        self.label_steps = tk.Label(top_frame, text="Steps: 0", width=12, anchor="w")
        self.label_steps.pack(side=tk.LEFT, padx=10)

        self.label_eps = tk.Label(top_frame, text="Epsilon: N/A", width=16, anchor="w")
        self.label_eps.pack(side=tk.LEFT, padx=10)

        self.label_sps = tk.Label(top_frame, text="Steps/sec: 0", width=16, anchor="w")
        self.label_sps.pack(side=tk.LEFT, padx=10)

        self.label_htile = tk.Label(top_frame, text="Highest Tile: 0", width=18, anchor="w")
        self.label_htile.pack(side=tk.LEFT, padx=10)

        self.best_score = 0  # Initialize best score
        self.label_best_score = tk.Label(top_frame, text="Best Score: 0", width=18, anchor="w")
        self.label_best_score.pack(side=tk.LEFT, padx=10)

        # -- Matplotlib Figure with 3 Subplots: Reward, Loss, Score --
        self.fig = Figure(figsize=(7, 4), dpi=100)
        # We use a 3x1 grid for three subplots
        self.ax_reward = self.fig.add_subplot(311)
        self.ax_score = self.fig.add_subplot(312)
        self.ax_loss = self.fig.add_subplot(313)
        self.fig.tight_layout()

        self.canvas = FigureCanvasTkAgg(self.fig, master=self)
        self.canvas_widget = self.canvas.get_tk_widget()
        self.canvas_widget.pack(side=tk.TOP, fill=tk.BOTH, expand=True)

        # Data for plots
        self.episodes_list = []
        self.rewards_list = []
        self.losses_list = []
        self.scores_list = [] 

        # Start the training in a background thread
        self.train_thread = threading.Thread(
            target=self.run_training_thread,
            daemon=True
        )
        self.train_thread.start()

        # Start polling the queue
        self.after(100, self.poll_queue)

    def run_training_thread(self):
        """
        This method runs in the background thread.
        We'll call a method from your AI (like "train_in_thread")
        that uses the hyperparams we set. We'll pass self.result_queue for updates.
        """
        # You might create a new method in DQN2048 that accepts all these parameters:
        self.ai.train_in_thread(
            env=self.env,
            episodes=self.episodes,
            gamma=self.gamma,
            lr_start=self.lr_start,
            lr_end=self.lr_end,
            batch_size=self.batch_size,
            memory_size=self.memory_size,
            target_update=self.target_update,
            eps_start=self.eps_start,
            eps_end=self.eps_end,
            eps_decay=self.eps_decay,
            temperature=self.temperature,
            result_queue=self.result_queue,
        )
    def poll_queue(self):
        """
        Check the result_queue for new training info.
        Update the UI. Schedule itself to run again after 100ms.
        """
        while True:
            try:
                msg = self.result_queue.get_nowait()
            except queue.Empty:
                break

            if msg is None:
                # Training is done
                tk.messagebox.showinfo("Training Complete", "All episodes finished!")
                return
            else:
                # Unpack data from the training thread
                episode = msg["episode"]
                episodes_total = msg["episodes_total"]
                steps_done = msg["steps_done"]
                steps_per_sec = msg["steps_per_sec"]
                highest_tile_overall = msg["highest_tile_overall"]
                epsilon = msg["epsilon"]
                reward = msg["reward"]
                loss = msg["loss"]
                score = msg["score"]  # Current game score

                # Update the best score if the current score is higher
                if score > self.best_score:
                    self.best_score = score
                    self.label_best_score.config(text=f"Best Score: {self.best_score}")

                # Update labels
                self.label_episode.config(text=f"Episode: {episode}/{episodes_total}")
                self.label_steps.config(text=f"Steps: {steps_done}")
                self.label_eps.config(text=f"Epsilon: {epsilon:.4f}")
                self.label_sps.config(text=f"Steps/sec: {steps_per_sec:.2f}")
                self.label_htile.config(text=f"Highest Tile: {highest_tile_overall}")

                # Update data lists
                self.episodes_list.append(episode)
                self.rewards_list.append(reward)
                self.losses_list.append(loss)
                self.scores_list.append(score)

                # Redraw all plots
                self.ax_reward.clear()
                self.ax_reward.set_title("Reward per Episode")
                self.ax_reward.set_xlabel("Episode")
                self.ax_reward.set_ylabel("Reward")
                self.ax_reward.plot(self.episodes_list, self.rewards_list, color="blue")

                self.ax_score.clear()
                self.ax_score.set_title("Score per Episode")
                self.ax_score.set_xlabel("Episode")
                self.ax_score.set_ylabel("Score")
                self.ax_score.plot(self.episodes_list, self.scores_list, color="green")

                self.ax_loss.clear()
                self.ax_loss.set_title("Loss per Episode")
                self.ax_loss.set_xlabel("Episode")
                self.ax_loss.set_ylabel("Loss")
                self.ax_loss.plot(self.episodes_list, self.losses_list, color="red")                

                self.fig.tight_layout()
                self.canvas.draw()

        # Continue polling
        self.after(100, self.poll_queue)

class TrainingConfigWindow(tk.Toplevel):
    """
    A window that lets the user set all the training parameters before starting training.
    Once the user clicks 'Start Training', we open the TrainingWindow with the specified hyperparams.
    """
    def __init__(self, parent):
        super().__init__(parent)
        self.title("Training Configuration")
        self.geometry("400x500")
        self.transient(parent)  # Make it a modal dialog
        self.grab_set()  # Ensure it captures all events while open
        self.resizable(False, False)

        # We'll store entries in a dict for convenience
        self.entries = {}

        row = 0

        # MODEL PATH (LOAD EXISTING)
        tk.Label(self, text="Model Path").grid(row=row, column=0, sticky="e", padx=5, pady=5)
        self.model_path_var = tk.StringVar()
        tk.Entry(self, textvariable=self.model_path_var, width=30).grid(row=row, column=1, padx=5, pady=5)
        tk.Button(self, text="Browse", command=self.browse_model).grid(row=row, column=2, padx=5, pady=5)
        row += 1

        # EPISODES
        row = self._add_label_entry("Episodes", default_val="5000", row=row)
        # GAMMA
        row = self._add_label_entry("Gamma", default_val="0.99", row=row)
        # LR_START
        row = self._add_label_entry("LR Start", default_val="0.0005", row=row)
        # LR_END
        row = self._add_label_entry("LR End", default_val="0.0005", row=row)
        # BATCH_SIZE
        row = self._add_label_entry("Batch Size", default_val="128", row=row)
        # MEMORY_SIZE
        row = self._add_label_entry("Memory Size", default_val="100000", row=row)
        # TARGET_UPDATE
        row = self._add_label_entry("Target Update (steps)", default_val="25", row=row)
        # EPS_START
        row = self._add_label_entry("Eps Start", default_val="1.0", row=row)
        # EPS_END
        row = self._add_label_entry("Eps End", default_val="0.1", row=row)
        # EPS_DECAY
        row = self._add_label_entry("Eps Decay", default_val="40000", row=row)
        # TEMPERATURE
        row = self._add_label_entry("Temperature", default_val="1.0", row=row)

        # START TRAINING BUTTON
        tk.Button(self, text="Start Training", command=self.start_training).grid(row=row, column=0, columnspan=3, pady=20)


    def _add_label_entry(self, label_text, default_val, row):
        """
        Utility to add a label + entry with default value.
        We'll store the StringVar in self.entries[label_text].
        """
        tk.Label(self, text=label_text).grid(row=row, column=0, sticky="e", padx=5, pady=5)
        var = tk.StringVar(value=default_val)
        entry = tk.Entry(self, textvariable=var, width=15)
        entry.grid(row=row, column=1, padx=5, pady=5)
        self.entries[label_text] = var
        return row + 1

    def browse_model(self):
        """
        Let user pick a model file. Store path in self.model_path_var.
        """
        file_path = filedialog.askopenfilename(
            title="Select Model File",
            filetypes=[("PyTorch Files", "*.pth"), ("All Files", "*.*")]
        )
        if file_path:
            self.model_path_var.set(file_path)

    def start_training(self):
        """
        Read all user entries, parse them, create a new AI (with model if provided),
        then open the TrainingWindow with the user-specified hyperparameters.
        """
        try:
            model_path = self.model_path_var.get().strip() or None

            episodes = int(self.entries["Episodes"].get().strip())
            gamma = float(self.entries["Gamma"].get().strip())
            lr_start = float(self.entries["LR Start"].get().strip())
            lr_end = float(self.entries["LR End"].get().strip())
            batch_size = int(self.entries["Batch Size"].get().strip())
            memory_size = int(self.entries["Memory Size"].get().strip())
            target_update = int(self.entries["Target Update (steps)"].get().strip())
            eps_start = float(self.entries["Eps Start"].get().strip())
            eps_end = float(self.entries["Eps End"].get().strip())
            eps_decay = float(self.entries["Eps Decay"].get().strip())
            temperature = float(self.entries["Temperature"].get().strip())
        except ValueError as e:
            messagebox.showerror("Invalid input", f"Please enter valid numeric values.\n\nError: {e}")
            return

        # Create environment
        env = GameEngine()

        # Create AI
        if model_path:
            ai = DQN2048(model_path=model_path)
        else:
            ai = DQN2048()

        # Create the TrainingWindow
        train_window = TrainingWindow(
            parent=self,
            ai=ai,
            env=env,
            episodes=episodes,
            gamma=gamma,
            lr_start=lr_start,
            lr_end=lr_end,
            batch_size=batch_size,
            memory_size=memory_size,
            target_update=target_update,
            eps_start=eps_start,
            eps_end=eps_end,
            eps_decay=eps_decay,
            temperature=temperature,
        )
        train_window.grab_set()  # Make the training window modal if needed



================================================
File: other/game_animation.py
================================================
import tkinter as tk
import random
from tkinter import messagebox

# Constants for the game
GRID_SIZE = 4
CELL_SIZE = 100
CELL_PADDING = 10
BACKGROUND_COLOR = "#92877d"
EMPTY_CELL_COLOR = "#9e948a"

CELL_COLORS = {
    2: "#eee4da",
    4: "#ede0c8",
    8: "#f2b179",
    16: "#f59563",
    32: "#f67c5f",
    64: "#f65e3b",
    128: "#edcf72",
    256: "#edcc61",
    512: "#edc850",
    1024: "#edc53f",
    2048: "#edc22e",
}

FONT = ("Verdana", 24, "bold")

# Adjust these for faster animations
ANIMATION_STEPS = 3     # Smaller means fewer frames => faster
ANIMATION_DELAY = 20    # Milliseconds between each animation step => faster

class Game2048:
    def __init__(self, master):
        self.master = master
        master.title("2048 Game")
        master.resizable(False, False)

        # Game state
        self.grid = [[0] * GRID_SIZE for _ in range(GRID_SIZE)]
        self.score = 0
        self.best_score = 0  # Keep track of best (high) score

        # If True, we do not accept new moves
        self.animation_in_progress = False

        # Top frame for Score, Best, and Restart button
        top_frame = tk.Frame(master)
        top_frame.pack(pady=10)

        self.score_label = tk.Label(top_frame, text=f"Score: {self.score}", font=("Verdana", 16))
        self.score_label.pack(side=tk.LEFT, padx=10)

        self.best_label = tk.Label(top_frame, text=f"Best: {self.best_score}", font=("Verdana", 16))
        self.best_label.pack(side=tk.LEFT, padx=10)

        self.restart_button = tk.Button(top_frame, text="Restart", font=("Verdana", 12), command=self.restart_game)
        self.restart_button.pack(side=tk.LEFT, padx=10)

        canvas_width = GRID_SIZE * (CELL_SIZE + CELL_PADDING) + CELL_PADDING
        canvas_height = GRID_SIZE * (CELL_SIZE + CELL_PADDING) + CELL_PADDING
        self.canvas = tk.Canvas(master, width=canvas_width, height=canvas_height, bg=BACKGROUND_COLOR)
        self.canvas.pack()

        self.draw_grid()
        self.add_random_tile()
        self.add_random_tile()
        self.draw_tiles()

        # Bind arrow keys
        master.bind("<Up>", self.move_up)
        master.bind("<Down>", self.move_down)
        master.bind("<Left>", self.move_left)
        master.bind("<Right>", self.move_right)

    # -------------------------------------------------------------------------
    #                   RESTART / NEW GAME BUTTON
    # -------------------------------------------------------------------------
    def restart_game(self):
        """Reset the game to an empty grid and reset score."""
        self.grid = [[0] * GRID_SIZE for _ in range(GRID_SIZE)]
        self.score = 0
        self.animation_in_progress = False
        self.score_label.config(text=f"Score: {self.score}")
        self.draw_grid()
        self.add_random_tile()
        self.add_random_tile()
        self.draw_tiles()

    # -------------------------------------------------------------------------
    #                   DRAWING THE BOARD AND TILES
    # -------------------------------------------------------------------------
    def draw_grid(self):
        self.canvas.delete("all")
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                x0 = CELL_PADDING + j * (CELL_SIZE + CELL_PADDING)
                y0 = CELL_PADDING + i * (CELL_SIZE + CELL_PADDING)
                x1 = x0 + CELL_SIZE
                y1 = y0 + CELL_SIZE
                self.canvas.create_rectangle(x0, y0, x1, y1, fill=EMPTY_CELL_COLOR, outline="")

    def draw_tiles(self):
        self.draw_grid()
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                value = self.grid[i][j]
                if value != 0:
                    self.draw_single_tile(i, j, value)

        # Update score labels
        self.score_label.config(text=f"Score: {self.score}")
        if self.score > self.best_score:
            self.best_score = self.score
        self.best_label.config(text=f"Best: {self.best_score}")

        self.master.update_idletasks()

    def draw_single_tile(self, i, j, value, offset_x=0, offset_y=0):
        """
        Draws a single tile at (i,j). 
        offset_x/offset_y are used for animation (pixel offsets).
        """
        x0 = CELL_PADDING + j * (CELL_SIZE + CELL_PADDING) + offset_x
        y0 = CELL_PADDING + i * (CELL_SIZE + CELL_PADDING) + offset_y
        x1 = x0 + CELL_SIZE
        y1 = y0 + CELL_SIZE

        color = CELL_COLORS.get(value, "#3c3a32")
        self.canvas.create_rectangle(x0, y0, x1, y1, fill=color, outline="")
        x_center = x0 + CELL_SIZE / 2
        y_center = y0 + CELL_SIZE / 2
        self.canvas.create_text(x_center, y_center, text=str(value), font=FONT, fill="#776e65")

    # -------------------------------------------------------------------------
    #                   ADD A RANDOM TILE
    # -------------------------------------------------------------------------
    def add_random_tile(self):
        empty_cells = [(i, j) for i in range(GRID_SIZE) for j in range(GRID_SIZE) if self.grid[i][j] == 0]
        if not empty_cells:
            return
        i, j = random.choice(empty_cells)
        self.grid[i][j] = 4 if random.random() < 0.1 else 2

    # -------------------------------------------------------------------------
    #       MOVE LOGIC (LEFT / RIGHT / UP / DOWN) + TRACKING TILE MOVEMENTS
    # -------------------------------------------------------------------------
    def move_left(self, event):
        if self.animation_in_progress:
            return
        self.animation_in_progress = True

        old_grid = [row[:] for row in self.grid]
        new_grid, moves = self.slide_left(old_grid)

        if new_grid == old_grid:  # No movement or merge
            self.animation_in_progress = False
            return

        self.animate_tiles(old_grid, new_grid, moves)
        self.grid = new_grid
        self.add_random_tile()
        self.draw_tiles()
        self.check_2048_tile()
        self.check_game_over()
        self.animation_in_progress = False

    def move_right(self, event):
        if self.animation_in_progress:
            return
        self.animation_in_progress = True

        old_grid = [row[:] for row in self.grid]
        reversed_grid = [row[::-1] for row in old_grid]
        slid_grid, moves = self.slide_left(reversed_grid)
        final_grid = [row[::-1] for row in slid_grid]

        fixed_moves = []
        for (r0, c0, r1, c1, val) in moves:
            old_c_fixed = GRID_SIZE - 1 - c0
            new_c_fixed = GRID_SIZE - 1 - c1
            fixed_moves.append((r0, old_c_fixed, r1, new_c_fixed, val))

        if final_grid == old_grid:
            self.animation_in_progress = False
            return

        self.animate_tiles(old_grid, final_grid, fixed_moves)
        self.grid = final_grid
        self.add_random_tile()
        self.draw_tiles()
        self.check_2048_tile()
        self.check_game_over()
        self.animation_in_progress = False

    def move_up(self, event):
        if self.animation_in_progress:
            return
        self.animation_in_progress = True

        old_grid = [row[:] for row in self.grid]
        transposed = self.transpose(old_grid)
        new_transposed, moves = self.slide_left(transposed)
        final_grid = self.transpose(new_transposed)

        fixed_moves = []
        for (r0, c0, r1, c1, val) in moves:
            fixed_moves.append((c0, r0, c1, r1, val))

        if final_grid == old_grid:
            self.animation_in_progress = False
            return

        self.animate_tiles(old_grid, final_grid, fixed_moves)
        self.grid = final_grid
        self.add_random_tile()
        self.draw_tiles()
        self.check_2048_tile()
        self.check_game_over()
        self.animation_in_progress = False

    def move_down(self, event):
        if self.animation_in_progress:
            return
        self.animation_in_progress = True

        old_grid = [row[:] for row in self.grid]
        transposed = self.transpose(old_grid)
        reversed_grid = [row[::-1] for row in transposed]
        slid_grid, moves = self.slide_left(reversed_grid)
        unreversed = [row[::-1] for row in slid_grid]
        final_grid = self.transpose(unreversed)

        fixed_moves = []
        for (r0, c0, r1, c1, val) in moves:
            c0_fixed = GRID_SIZE - 1 - c0
            c1_fixed = GRID_SIZE - 1 - c1
            fixed_moves.append((c0_fixed, r0, c1_fixed, r1, val))

        if final_grid == old_grid:
            self.animation_in_progress = False
            return

        self.animate_tiles(old_grid, final_grid, fixed_moves)
        self.grid = final_grid
        self.add_random_tile()
        self.draw_tiles()
        self.check_2048_tile()
        self.check_game_over()
        self.animation_in_progress = False

    # -------------------------------------------------------------------------
    #     slide_left: returns (new_grid, moves_list) describing tile movements
    # -------------------------------------------------------------------------
    def slide_left(self, old_grid):
        new_grid = []
        moves = []

        for row_idx in range(GRID_SIZE):
            old_row = old_grid[row_idx]

            # Gather non-zero tiles => list of (orig_col, value)
            temp = []
            for col_idx, val in enumerate(old_row):
                if val != 0:
                    temp.append((col_idx, val))

            final_row = [0] * GRID_SIZE
            write_pos = 0
            skip_next = False
            for i in range(len(temp)):
                if skip_next:
                    skip_next = False
                    continue

                col_i, val_i = temp[i]
                # Check if next tile can merge
                if i < len(temp) - 1 and val_i == temp[i+1][1]:
                    col_j, val_j = temp[i+1]
                    merged_val = val_i * 2
                    self.score += merged_val

                    # Merge: tile col_j merges into position `write_pos`
                    moves.append((row_idx, col_j, row_idx, write_pos, merged_val))
                    # If the left tile col_i also physically moved, track that
                    if col_i != write_pos:
                        moves.append((row_idx, col_i, row_idx, write_pos, merged_val))

                    final_row[write_pos] = merged_val
                    skip_next = True
                else:
                    # No merge => tile moves from col_i to write_pos
                    if col_i != write_pos:
                        moves.append((row_idx, col_i, row_idx, write_pos, val_i))
                    final_row[write_pos] = val_i

                write_pos += 1

            new_grid.append(final_row)

        return new_grid, moves

    # -------------------------------------------------------------------------
    #               ANIMATION: ONLY FOR TILES IN 'moves'
    # -------------------------------------------------------------------------
    def animate_tiles(self, old_grid, new_grid, moves):
        for step in range(1, ANIMATION_STEPS + 1):
            t = step / ANIMATION_STEPS

            self.canvas.delete("all")
            self.draw_grid()

            # 1. Draw tiles that are NOT moving
            moving_positions = set((m[2], m[3]) for m in moves)  # (new_i, new_j)
            original_positions = set((m[0], m[1]) for m in moves)

            for i in range(GRID_SIZE):
                for j in range(GRID_SIZE):
                    val_final = new_grid[i][j]
                    # Draw if not in motion
                    if val_final != 0 and (i, j) not in moving_positions:
                        if (i, j) not in original_positions:
                            self.draw_single_tile(i, j, val_final)

            # 2. Draw moving tiles with interpolation
            for (old_i, old_j, new_i, new_j, val) in moves:
                row_diff = new_i - old_i
                col_diff = new_j - old_j
                offset_y = row_diff * (CELL_SIZE + CELL_PADDING) * t
                offset_x = col_diff * (CELL_SIZE + CELL_PADDING) * t
                self.draw_single_tile(old_i, old_j, val, offset_x=offset_x, offset_y=offset_y)

            self.master.update_idletasks()
            self.master.after(ANIMATION_DELAY)

    # -------------------------------------------------------------------------
    #          CHECK FOR 2048 TILE / CHECK GAME OVER
    # -------------------------------------------------------------------------
    def check_2048_tile(self):
        for row in self.grid:
            if 2048 in row:
                messagebox.showinfo("2048", "Congratulations! You created a 2048 tile!")
                return

    def check_game_over(self):
        for i in range(GRID_SIZE):
            for j in range(GRID_SIZE):
                if self.grid[i][j] == 0:
                    return
                if j < GRID_SIZE - 1 and self.grid[i][j] == self.grid[i][j + 1]:
                    return
                if i < GRID_SIZE - 1 and self.grid[i][j] == self.grid[i + 1][j]:
                    return

        messagebox.showinfo("2048", f"Game Over!\nYour Score: {self.score}")
        self.master.quit()

    def transpose(self, grid):
        return [list(row) for row in zip(*grid)]


def main():
    root = tk.Tk()
    game = Game2048(root)
    root.mainloop()

if __name__ == "__main__":
    main()


================================================
File: utils/gpu_available.py
================================================
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


